[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robin Reese",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "{{&lt; pdf files/resume.pdf width=100% height=800 &gt;}}"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nRobin Reese\n\n\nApr 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTest Page 2\n\n\n\n\n\n\nYour Name\n\n\nApr 29, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn this field experiment involving over 50,000 prior donors to a non-profit, the researchers investigate how offering matching grants affects charitable giving. Two-thirds of the sample were placed into a treatment group, receiving solicitation letters highlighting that their donations would be matched by an anonymous donor at various ratios and maximum amounts, while the remaining third, serving as a control group, received standard solicitation materials without mention of a match. The treatment group was further stratified into subgroups, each receiving tailored messaging about the matching ratio, the cap on the matching grant, and suggested donation amounts based on their previous giving history.\nThe study‚Äôs key outcome measures were the response rate to the solicitation and the amount of money donated. By integrating subtle variations in the solicitation letter and the accompanying reply card, the experiment meticulously isolates the influence of the matching grant information. The results offer actionable insights into the effectiveness of different fundraising strategies, revealing the intricate ways in which donors‚Äô decisions are influenced not only by the economic benefit of matching grants but also by their previous engagement levels and the manner in which the opportunity to have their donation matched is communicated.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nWe‚Äôll start this investigation by loading the data and getting our various tools ready.\n\n\nCode\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in data\ndf = pd.read_csv('blueprinty.csv')\nprint(df.head())\n\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\nOne of the first items to check is the general distribution of patents held within the firm data.\n\n\nCode\n# Group the data by customer status\ngrouped = df.groupby('iscustomer')\n\n# Plot histograms and add the mean bar as a vertical line\ngrouped['patents'].plot(kind='hist', alpha=0.5, legend=True)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.show()\n\n# Calculate means\nmeans = grouped['patents'].mean()\nprint(means)\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nAt a glance, it does appear that those firms who are also customers of Blueprinty have a higher average number of patents. From the above we see that non-customers hold roughly 3.6 patents whereas customers hold 4.1 patents on average.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs plot the distribution of ages and location to see if anything stands out.\n\n\nCode\n# Create a new column for age bracket\ndf['agebracket'] = pd.cut(df['age'], bins=range(0, 51, 10), right=False)\n\n# Show the counts by customer status\nsns.countplot(x='agebracket', hue='iscustomer', data=df)\nplt.xlabel('Age Bracket')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Age Bracket')\nplt.show()\n\n# Print the mean customer status by age bracket\nprint(df.groupby('agebracket')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nagebracket\n[0, 10)     0.000000\n[10, 20)    0.219269\n[20, 30)    0.122708\n[30, 40)    0.083141\n[40, 50)    0.145455\nName: iscustomer, dtype: float64\n\n\n\n\nCode\n# Show the count of each region by customer status\nsns.countplot(x='region', hue='iscustomer', data=df)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Region')\nplt.show()\n\n# print the mean customer status by region\nprint(df.groupby('region')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nregion\nMidwest      0.075893\nNortheast    0.188020\nNorthwest    0.085561\nSouth        0.104712\nSouthwest    0.104377\nName: iscustomer, dtype: float64\n\n\nThere is definitely a skew in the data. Many firms are within the age range of 10 - 30 years. The region field also suggests that the firms that are also customers are heavily segmented in the Northeast region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nSimple Poison Model: \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nGiven the above, the expression\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\log \\left(\\prod_{i=1}^n y_i!\\right)\n\\]\nis the log-likelihood function for a set of ùëõ observations assumed to be independently and identically distributed according to a Poisson distribution with parameter Œª.\nIn Python that math would take the form of the following function:\n\n\nCode\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda values\n    # Calculate the log-likelihood\n    try:\n        log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([np.log(math.factorial(y)) for y in Y])\n    except OverflowError:  # Handling OverflowError that can occur with large factorials\n        log_likelihood = float('-inf')\n    return log_likelihood\n\n\nNow we can use that function to plot the log-likelihood against alues of lambda. The maximum likelihood estimator is shown at the peak of the curve. We can see that it takes the value of approximately 3.68 which you may remember is somewhat closer to the average number of patents of non-customers.\n\n\nCode\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, Y.max() + 2, 400)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]\n\n# Calculate Y_bar, the sample mean of the observed data Y\nY_bar = np.mean(Y)\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.axvline(x=Y_bar, color='red', linestyle='--', label=f'Y_bar at {Y_bar:.2f}', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda Values')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html#sub-header",
    "href": "projects/project2/index.html#sub-header",
    "title": "Test Page 2",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#introduction",
    "href": "projects/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn this field experiment involving over 50,000 prior donors to a non-profit, the researchers investigate how offering matching grants affects charitable giving. Two-thirds of the sample were placed into a treatment group, receiving solicitation letters highlighting that their donations would be matched by an anonymous donor at various ratios and maximum amounts, while the remaining third, serving as a control group, received standard solicitation materials without mention of a match. The treatment group was further stratified into subgroups, each receiving tailored messaging about the matching ratio, the cap on the matching grant, and suggested donation amounts based on their previous giving history.\nThe study‚Äôs key outcome measures were the response rate to the solicitation and the amount of money donated. By integrating subtle variations in the solicitation letter and the accompanying reply card, the experiment meticulously isolates the influence of the matching grant information. The results offer actionable insights into the effectiveness of different fundraising strategies, revealing the intricate ways in which donors‚Äô decisions are influenced not only by the economic benefit of matching grants but also by their previous engagement levels and the manner in which the opportunity to have their donation matched is communicated.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/index.html#data",
    "href": "projects/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe first step in our analysis of the data will be to load and explore the results. Below we bring in the state file and take a quick overview of the variables‚Äô counts, types, and distributions.\n\n\nCode\n# Read the .dta file\nimport pandas as pd\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# Get an overview of the data\ndf.info()\n\n# Describe the data\ndata_description = df.describe()\nprint(data_description)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168560      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378105     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258633  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n::::\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nDirect t-tests and t-tests from linear regression are both statistical tools used to assess the significance of differences between groups or the impact of predictors. A direct t-test compares the means of two independent samples to determine if they come from distributions with equal means, making it ideal for straightforward comparisons between two groups, such as control and treatment conditions in an experiment. The resulting p-value indicates whether any observed difference is likely to be due to chance.\nIn contrast, a t-test from a linear regression analysis evaluates the significance of individual predictors within a more complex model that may include multiple variables. The t-statistic here assesses whether a coefficient differs significantly from zero, taking into account other factors in the model. This allows for the evaluation of each predictor‚Äôs unique contribution and the control of confounding variables. Both types of t-tests rely on the assumption of normally distributed errors and can be adapted for equal or unequal variances between groups. While direct t-tests are best suited for simpler experimental designs, regression t-tests excel in multifaceted studies where multiple influences need to be accounted for simultaneously.\nWe‚Äôll define functions next that are able to perform both test methodologies on our dataset as needed.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef direct_t_test(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data for consistency with regression\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Separate the treatment and control groups\n    treatment_group = df[df[treatment_col] == 1][outcome_col]\n    control_group = df[df[treatment_col] == 0][outcome_col]\n\n    # Calculate means\n    mean_treatment = treatment_group.mean()\n    mean_control = control_group.mean()\n\n    # Calculate standard deviations\n    std_treatment = treatment_group.std(ddof=1)\n    std_control = control_group.std(ddof=1)\n\n    # Calculate sample sizes\n    n_treatment = len(treatment_group)\n    n_control = len(control_group)\n\n    # Calculate separate standard errors\n    se_treatment = std_treatment / np.sqrt(n_treatment)\n    se_control = std_control / np.sqrt(n_control)\n\n    # Calculate the t-statistic\n    t_stat = (mean_treatment - mean_control) / np.sqrt(se_treatment**2 + se_control**2)\n\n    # Calculate degrees of freedom using the Welch-Satterthwaite equation\n    df = ((se_treatment**2 + se_control**2)**2 /\n          ((se_treatment**4 / (n_treatment - 1)) + (se_control**4 / (n_control - 1))))\n\n    return t_stat, df\n\ndef run_regression(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Prepare the design matrix X with a constant (intercept) and the treatment indicator\n    X = sm.add_constant(df[treatment_col])\n    Y = df[outcome_col]\n\n    # Fit the OLS regression model\n    model = sm.OLS(Y, X).fit()\n\n    # Extract the t-statistic and p-value for the treatment variable\n    t_stat = model.tvalues[treatment_col]\n\n    # Return the t-statistic and degrees of freedom\n    return t_stat, model.df_resid\n\n# Example usage (you need to replace 'your_dataframe', 'treatment', and 'outcome' with your actual DataFrame and column names)\n# t_stat_direct, df_direct = direct_t_test(your_dataframe, 'treatment', 'outcome')\n# t_stat_regression, df_regression = run_regression(your_dataframe, 'treatment', 'outcome')\n# print(\"Direct t-test:\", t_stat_direct, \"df:\", df_direct)\n# print(\"Regression t-test:\", t_stat_regression, \"df:\", df_regression)\n\n\nNow that the functions are defined, we can test different variables besides the outcome variables to see if the treatment and control groups show significant differences from each other.\n\n\nCode\n# Prepare groups for direct t-test of 'mrm2'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'mrm2')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'mrm2')\nprint('Testing the mrm2 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'female'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'female')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'female')\nprint('Testing the female variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'red0'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'red0')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'red0')\nprint('Testing the red0 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'years')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'years')\nprint('Testing the years variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n\nTesting the mrm2 variable\nDirect t-test: 0.11953155228176904\nRegression t-test: 0.11949210581591684\nTesting the female variable\nDirect t-test: -1.7535132542518672\nRegression t-test: -1.7583691871450848\nTesting the red0 variable\nDirect t-test: 1.877281316440582\nRegression t-test: 1.8750884882812506\nTesting the years variable\nDirect t-test: -1.0909175279573782\nRegression t-test: -1.1030383745788988"
  },
  {
    "objectID": "projects/project1/index.html#experimental-results",
    "href": "projects/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nAnalyzing whether matched donations lead to an increased response rate of making a donation is a critical first step in reproducing the results of a study because it directly assesses the effectiveness of the matching incentive as a motivational tool in charitable giving. This analysis establishes the foundational evidence needed to validate the underlying assumptions of fundraising strategies that utilize matching offers to enhance donor engagement and generosity. Below we‚Äôll plot the differences in response rate between the treatment and control groups.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Calculate the proportions for the 'gave' column based on the 'treatment' indicator\nproportions = df.groupby('treatment')['gave'].mean()\n\n# Create a barplot\nplt.figure(figsize=(10, 6))\ncolors = ['#1f77b4', '#ff7f0e']  # Aesthetically pleasing color palette\nproportions.plot(kind='bar', color=colors)\n\n# Add labels and title\nplt.title('Proportion of People Who Donated by Group')\nplt.xlabel('Group')\nplt.ylabel('Proportion of Donations')\nplt.xticks(ticks=[0, 1], labels=['Control', 'Treatment'], rotation=0)  # Rename x-ticks for clarity\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nIt appears from a visualization that there is a difference. The treatment group has a slightly higher response rate than does the control group. How sure are we that this effect size is statistically significant rather than due to random chance?\nTo determine this the next step is to run a t-test between the treatment and control groups on the donation outcome. Using the binary variable ‚Äògave‚Äô. This designates whether a person gave a donation in any amount.\n\n\nCode\nfrom scipy import stats\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'gave')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'gave')\nprint('Testing the gave variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n\n\nTesting the gave variable\nDirect t-test: 3.2094621908279835\nRegression t-test: 3.101361000543946\nP-value: 0.001330982345091547\n\n\nThe above results show stastical values that lead one to believe the difference in response rates between the treatment and control groups are unlikely to have occurred by chance. It indicates that such financial incentives can effectively influence human behavior, enhancing the likelihood of donating or possibly increasing the donation amounts.\n\n\nCode\nimport statsmodels.formula.api as smf\n\n# Run the probit regression model\ndef run_probit_regression(df, formula):\n    # Probit model using the formula interface\n    model = smf.probit(formula, data=df)\n    results = model.fit(disp=0)\n    return results\n\n# Use the probit regression function\nformula = 'gave ~ treatment'\nprobit_results = run_probit_regression(df, formula)\n\n# Print out the summary of the regression results\ncoefficients = probit_results.params\np_values = probit_results.pvalues\nprint(\"Treatment Coefficient: \", coefficients[1])\nprint(\"Treatment P-Value: \", p_values[1])\n\n\nTreatment Coefficient:  0.08678462244745781\nTreatment P-Value:  0.0018523990147786633\n\n\n\n\nDifferences between Match Rates\nIn the realm of charitable giving, the size of matched donations often plays a pivotal role in incentivizing potential donors. Understanding how different matching ratios influence donor behavior can provide valuable insights for optimizing fundraising strategies. This analysis focuses on evaluating the impact of various match rates‚Äî1:1, 2:1, and 3:1‚Äîon the likelihood of donations. By employing a series of t-tests, the study seeks to determine if higher match ratios significantly increase the response rate among donors, thereby testing the assumption that more generous matching offers might lead to higher participation rates in donation campaigns.\nBelow is the code to assess the differences in rates of response between different matching offers presented:\n\n\nCode\nfrom scipy import stats\n\n# Calculate the mean donation rate for each ratio category\nmean_ratio1 = df[df['ratio'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Conduct t-tests\nt_test_1_vs_2 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio2'] == 1]['gave'], equal_var=False)\nt_test_1_vs_3 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\nt_test_2_vs_3 = stats.ttest_ind(df[df['ratio2'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\n\n# Print the results\nprint(f\"Mean Donation Rate for 1:1 match: {mean_ratio1:.4f}\")\nprint(f\"Mean Donation Rate for 2:1 match: {mean_ratio2:.4f}\")\nprint(f\"Mean Donation Rate for 3:1 match: {mean_ratio3:.4f}\")\nprint(\"T-test Results:\")\nprint(\"1:1 vs 2:1:\", t_test_1_vs_2)\nprint(\"1:1 vs 3:1:\", t_test_1_vs_3)\nprint(\"2:1 vs 3:1:\", t_test_2_vs_3)\n\n\nMean Donation Rate for 1:1 match: 0.0207\nMean Donation Rate for 2:1 match: 0.0226\nMean Donation Rate for 3:1 match: 0.0227\nT-test Results:\n1:1 vs 2:1: TtestResult(statistic=-0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836)\n1:1 vs 3:1: TtestResult(statistic=-1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684)\n2:1 vs 3:1: TtestResult(statistic=-0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778)\n\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors‚Äô comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\nNext we‚Äôll use regression to look at the comparison of mean response rates for different matching ratios on whether people chose to donate. The coefficient of each variable [ratio1, ratio2, ratio3] can be looked at to determine the effect size between levels of matching presented. The analysis gives us a p-value as well to determine the significance of the movement suggested by the coefficient.\n\n\nCode\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n# Prepare the design matrix X with a constant and the dummy variables for ratio\nX = sm.add_constant(df[['ratio1', 'ratio2', 'ratio3']])\nY = df['gave']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n\n\n{'const': {'Coefficient': 0.01785821298016419, 'P-value': 4.7869277141173614e-59}, 'ratio1': {'Coefficient': 0.002890911245112018, 'P-value': 0.09662209260247782}, 'ratio2': {'Coefficient': 0.004775162266827, 'P-value': 0.006062639303220395}, 'ratio3': {'Coefficient': 0.004875186247079949, 'P-value': 0.005086912091717824}}\n\n\nInteresting. The summary output will be returned to shortly. Next we‚Äôll compare the mean response rates between the matching offers. This will show how much the response rate moves as the offer is increased from 1:1 to 2:1 to 3:1.\n\n\nCode\nmean_ratio1 = df[df['ratio1'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Calculate the differences directly from the data\ndifference_1_to_2 = mean_ratio2 - mean_ratio1\ndifference_2_to_3 = mean_ratio3 - mean_ratio2\n\nprint(f\"Directly calculated difference in response rate from 1:1 to 2:1 match ratio: {difference_1_to_2:.4f}\")\nprint(f\"Directly calculated difference in response rate from 2:1 to 3:1 match ratio: {difference_2_to_3:.4f}\")\n\n\nDirectly calculated difference in response rate from 1:1 to 2:1 match ratio: 0.0019\nDirectly calculated difference in response rate from 2:1 to 3:1 match ratio: 0.0001\n\n\nFor housekeeping we should look to see if the model coefficients show a similar movement to the mean comparisons.\n\n\nCode\ncoefficients = model.params\n\n# Calculate the difference between ratio2 - ratio1 and ratio3 - ratio2\ndifference_ratio2_ratio1 = coefficients['ratio2'] - coefficients['ratio1']\ndifference_ratio3_ratio2 = coefficients['ratio3'] - coefficients['ratio2']\n\ndifference_ratio2_ratio1, difference_ratio3_ratio2\n\nprint(f\"Coefficient comparison difference in response rate from 1:1 to 2:1 match ratio: {difference_ratio2_ratio1:.4f}\")\nprint(f\"Coefficient comparison difference in response rate from 2:1 to 3:1 match ratio: {difference_ratio3_ratio2:.4f}\")\n\n\nCoefficient comparison difference in response rate from 1:1 to 2:1 match ratio: 0.0019\nCoefficient comparison difference in response rate from 2:1 to 3:1 match ratio: 0.0001\n\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors‚Äô comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution. In order to do this we shall run the same t-test only this time we‚Äôre using the donation amount relative to the treatment effect.\n\n\nCode\n# Perform both t-tests of 'amount'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'amount')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'amount')\nprint('Testing the amount variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n\n\nTesting the amount variable\nDirect t-test: 1.9182283233541\nRegression t-test: 1.860502691500859\nP-value: 0.0550899208482003\n\n\nThe analysis suggests that matched donations have a potentially positive, though not statistically significant, impact on donation amounts at the traditional 5% significance level. Given the proximity of the p-value to this threshold, organizations might still consider matched donations as part of a broader, diversified fundraising strategy. Barring additional research, it may yet prove a positive influence on donation amounts.\nAs a next step we‚Äôll filter the data to only show those who donated something. This can then be compared to the treatment variable to see if treatment has an effect on the amount given that can be teased out of the data.\n\n\nCode\n# Filter to include only rows where a positive donation was made\ndf_donors = df[df['amount'] &gt; 0]\n\n# Prepare the design matrix X with a constant and the treatment indicator\nX = sm.add_constant(df_donors['treatment'])\nY = df_donors['amount']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n\n\n{'const': {'Coefficient': 45.54026845637584, 'P-value': 5.473577513353547e-68}, 'treatment': {'Coefficient': -1.6683934553392588, 'P-value': 0.5614755766155095}}\n\n\nThe coefficient for the treatment variable, which is negative, suggests that being in the treatment group (i.e., offered a matched donation) is associated with a decrease in the donation amount by approximately $1.67 compared to the control group. However, the p-value associated with this coefficient is 0.561, indicating that this effect is not statistically significant.\nThe regression analysis reveals that, contrary to expectations, the treatment (matched donations) does not lead to an increase in the amount donated. Instead, there‚Äôs an indication (though not statistically significant) that it might decrease the amount donated if we are to read the outputs of the regression. All in all, there is not enough information here to say that donation amount specifically is moved by the treatment.\n\n\nCode\ntreatment_donated = df_donors[df_donors['treatment'] == 1]\ncontrol_donated = df_donors[df_donors['treatment'] == 0]\n\n# Calculate the mean donation amount for each group\nmean_treatment = treatment_donated['amount'].mean()\nmean_control = control_donated['amount'].mean()\n\n# Creating histograms\nplt.figure(figsize=(14, 6))\n\n# Histogram for treatment group\nplt.subplot(1, 2, 1)\nplt.hist(treatment_donated['amount'], bins=30, color='blue', alpha=0.7)\nplt.axvline(mean_treatment, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Treatment Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_treatment:.2f}', xy=(mean_treatment, 50),\n             xytext=(mean_treatment + 50, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='right')\n\n# Histogram for control group\nplt.subplot(1, 2, 2)\nplt.hist(control_donated['amount'], bins=30, color='green', alpha=0.7)\nplt.axvline(mean_control, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Control Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_control:.2f}', xy=(mean_control, 20),\n             xytext=(mean_control - 150, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='left')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/project1/index.html#simulation-experiment",
    "href": "projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nIn the provided text, the author outlines an educational simulation designed to illustrate key statistical principles, namely the Law of Large Numbers and the Central Limit Theorem, using a practical example from a charitable donation context. By setting up a scenario where the probabilities of making a donation differ between respondents who receive a match and those who do not, the simulation aims to show how differences in probabilities influence donation behaviors over a large number of trials. Specifically, respondents who do not receive a match have a lower probability (0.018) of donating compared to those who receive a match (0.022).\nThe code below is structured to generate a substantial number of simulations‚Äî10,000 for each group‚Äîto model donation outcomes according to the specified Bernoulli distributions. It uses Python‚Äôs numpy library to simulate these outcomes, ensuring reproducibility by setting a random seed. Once the donation data for both control (no match) and treatment (match) groups are simulated, the script calculates the cumulative average of the differences in donation probabilities between the two groups across the number of draws. This method allows the plot to visually depict how the average differences evolve as more data points are considered, highlighting the convergence behavior predicted by the Law of Large Numbers.\n\n\nCode\n# Set the true probabilities for control and treatment\ntrue_prob_control = 0.018\ntrue_prob_treatment = 0.022\n\n# Number of simulations/draws\nnum_simulations = 10000\n\n# Simulate donations for control and treatment groups\nnp.random.seed(0)  # For reproducibility\ncontrol_donations = np.random.binomial(1, true_prob_control, num_simulations)\ntreatment_donations = np.random.binomial(1, true_prob_treatment, num_simulations)\n\n# Compute the cumulative average of the differences\ncumulative_differences = np.cumsum(treatment_donations - control_donations) / np.arange(1, num_simulations + 1)\n\n# Plot the cumulative averages\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_differences, color='red', lw=2)\nplt.axhline(y=true_prob_treatment - true_prob_control, color='blue', lw=1, linestyle='--')\n\nplt.xlabel('Number of Draws')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Cumulative Average Difference in Donation Rate')\nplt.show()\n\nprint(\"The mean difference is \" + str(true_prob_treatment - true_prob_control) + \" (the blue hash line).\")\n\n\n\n\n\n\n\n\n\nThe mean difference is 0.004 (the blue hash line).\n\n\nThe plot generated by the code visually confirms the theoretical expectation: as the number of simulations increases, the cumulative average of the differences should approach the true difference in means (0.004) between the control and treatment groups. This is depicted on the plot by the blue dashed line at the level of the true mean difference. The exercise not only reinforces the statistical theory behind sampling distributions and their averages but also provides an intuitive grasp of how small probability differences can be detected and quantified with sufficient data, underlining the practical applications of these concepts in analyzing and interpreting data from real-world experiments. This simulation thereby serves as a powerful tool in both teaching and understanding statistical inference through a direct and engaging approach.\n\n\nCentral Limit Theorem\nThe Central Limit Theorem (CLT) is a fundamental principle in statistics that plays a pivotal role in the simulation above. It states that, regardless of the distribution of the population, the distribution of the sample means will approximate a normal distribution as the sample size increases, provided the samples are independent and identically distributed with a finite mean and variance. This theorem is crucial because it justifies the use of normal probability theory in the inference about the mean of a population, even when the population itself is not normally distributed.\nIn this scenario, the Central Limit Theorem comes into play by ensuring that the distribution of the cumulative average differences between the treatment and control groups will approach a normal distribution as the number of draws (i.e., sample size) increases. Each draw represents a Bernoulli trial where a donation is made with a certain probability. As you simulate more trials, the average of these results (due to the law of large numbers) will converge not only towards the true mean difference but also the distribution of these averages will start to resemble a normal distribution (thanks to the CLT).\n\n\nCode\n# Let's generate the histograms for the sample sizes specified and comment on their distribution.\n\n# Function to simulate the process and calculate the averages\ndef simulate_averages(sample_size, repetitions, p_control, p_treatment):\n    control_means = np.random.binomial(sample_size, p_control, repetitions) / sample_size\n    treatment_means = np.random.binomial(sample_size, p_treatment, repetitions) / sample_size\n    return treatment_means - control_means\n\n# Sample sizes to generate histograms for\nsample_sizes = [50, 200, 500, 1000]\n\n# Control and treatment probabilities\np_control = 0.018\np_treatment = 0.022\n\n# Number of repetitions to calculate averages\nrepetitions = 1000\n\n# Set up the plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# Generate the histograms\nfor i, sample_size in enumerate(sample_sizes):\n    # Simulate the differences in averages\n    avg_differences = simulate_averages(sample_size, repetitions, p_control, p_treatment)\n    \n    # Plot histogram horizontally\n    axes[i].hist(avg_differences, bins=30, orientation='horizontal', color='skyblue', edgecolor='black')\n    mean_value = np.mean(avg_differences)\n    axes[i].axhline(y=mean_value, color='red', linestyle='dashed', linewidth=2)\n    \n    # Annotate the mean difference\n    axes[i].annotate(f'Mean: {mean_value:.4f}', xy=(0.5, mean_value), xytext=(10, 0), \n                     textcoords='offset points', fontsize=10, color='red', ha='center', va='bottom')\n    \n    # Set title and labels\n    axes[i].set_title(f'Sample size: {sample_size}')\n    axes[i].set_ylabel('Average Difference')\n    axes[i].set_xlabel('Frequency')\n\n# Adjust the layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFor each histogram corresponding to different sample sizes (50, 200, 500, 1000), zero should ideally be within the tails of the distribution if there is no true difference.\nSince the true probabilities differ by 0.004 (p_treatment - p_control), zero is not expected to be in the center if the simulation reflects the true difference.\nAs sample size increases, the distribution of the average differences should become more centered around the true difference (0.004), and the variance should decrease, making the distribution narrower around the mean. Though there is an element of randomness in each sample, we can see that as n increases the peak of the histogram starts to converge around the mean value we know to be 0.004."
  },
  {
    "objectID": "projects/project2/index.html#blueprinty-case-study",
    "href": "projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nWe‚Äôll start this investigation by loading the data and getting our various tools ready.\n\n\nCode\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in data\ndf = pd.read_csv('blueprinty.csv')\nprint(df.head())\n\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\nOne of the first items to check is the general distribution of patents held within the firm data.\n\n\nCode\n# Group the data by customer status\ngrouped = df.groupby('iscustomer')\n\n# Plot histograms and add the mean bar as a vertical line\ngrouped['patents'].plot(kind='hist', alpha=0.5, legend=True)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.show()\n\n# Calculate means\nmeans = grouped['patents'].mean()\nprint(means)\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nAt a glance, it does appear that those firms who are also customers of Blueprinty have a higher average number of patents. From the above we see that non-customers hold roughly 3.6 patents whereas customers hold 4.1 patents on average.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs plot the distribution of ages and location to see if anything stands out.\n\n\nCode\n# Create a new column for age bracket\ndf['agebracket'] = pd.cut(df['age'], bins=range(0, 51, 10), right=False)\n\n# Show the counts by customer status\nsns.countplot(x='agebracket', hue='iscustomer', data=df)\nplt.xlabel('Age Bracket')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Age Bracket')\nplt.show()\n\n# Print the mean customer status by age bracket\nprint(df.groupby('agebracket')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nagebracket\n[0, 10)     0.000000\n[10, 20)    0.219269\n[20, 30)    0.122708\n[30, 40)    0.083141\n[40, 50)    0.145455\nName: iscustomer, dtype: float64\n\n\n\n\nCode\n# Show the count of each region by customer status\nsns.countplot(x='region', hue='iscustomer', data=df)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Region')\nplt.show()\n\n# print the mean customer status by region\nprint(df.groupby('region')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nregion\nMidwest      0.075893\nNortheast    0.188020\nNorthwest    0.085561\nSouth        0.104712\nSouthwest    0.104377\nName: iscustomer, dtype: float64\n\n\nThere is definitely a skew in the data. Many firms are within the age range of 10 - 30 years. The region field also suggests that the firms that are also customers are heavily segmented in the Northeast region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nSimple Poison Model: \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nGiven the above, the expression\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\log \\left(\\prod_{i=1}^n y_i!\\right)\n\\]\nis the log-likelihood function for a set of ùëõ observations assumed to be independently and identically distributed according to a Poisson distribution with parameter Œª.\nIn Python that math would take the form of the following function:\n\n\nCode\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda values\n    # Calculate the log-likelihood\n    try:\n        log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([np.log(math.factorial(y)) for y in Y])\n    except OverflowError:  # Handling OverflowError that can occur with large factorials\n        log_likelihood = float('-inf')\n    return log_likelihood\n\n\nNow we can use that function to plot the log-likelihood against alues of lambda. The maximum likelihood estimator is shown at the peak of the curve. We can see that it takes the value of approximately 3.68 which you may remember is somewhat closer to the average number of patents of non-customers.\n\n\nCode\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, Y.max() + 2, 400)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]\n\n# Calculate Y_bar, the sample mean of the observed data Y\nY_bar = np.mean(Y)\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.axvline(x=Y_bar, color='red', linestyle='--', label=f'Y_bar at {Y_bar:.2f}', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda Values')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html#mathematical-derivation",
    "href": "projects/project2/index.html#mathematical-derivation",
    "title": "Poisson Regression Examples",
    "section": "Mathematical Derivation",
    "text": "Mathematical Derivation\nFor a set of observations \\(( Y = y_1, y_2, \\ldots, y_n )\\) that are independently and identically distributed according to a Poisson distribution, the log-likelihood function \\(\\lambda\\) is given by:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right)\n\\]\nTaking the derivative with respect to \\(\\lambda\\) and setting it to zero:\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( \\frac{y_i}{\\lambda} - 1 \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n\\]\nSetting this derivative to zero for maximization:\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i = n\n\\]\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{Y}\n\\]\nThis results in \\(\\lambda\\) being equal to \\(\\bar{Y}\\), which confirms that the mean of the Poisson distribution \\(\\lambda\\) is also the rate parameter that maximizes the likelihood of observing the given data (i.e.¬†the top of the curve on our plot).\nWe can also find this peak value by inputting this function into Python‚Äôs minimize function. Keep in mind that we are trying to maximize the function so instead we‚Äôll pass the negative log-likelihood.\n\n\nCode\n# Define the negative log-likelihood function\ndef neg_poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the negative log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_[0] &lt;= 0:\n        return np.inf  # Return infinity if lambda is not positive\n    return -np.sum(Y * np.log(lambda_[0]) - lambda_[0] - gammaln(Y + 1))\n\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Initial guess for lambda\ninitial_lambda = np.array([4.5]) \n\n# Using 'minimize' from scipy.optimize to find the MLE for lambda\nresult = minimize(neg_poisson_loglikelihood, x0=initial_lambda, args=(Y,), bounds=[(0.001, None)])\n\n# The optimal lambda found\nlambda_mle = result.x[0]\n\nprint(\"The MLE for lambda is:\", lambda_mle)\n\n\nThe MLE for lambda is: 3.68466651967392\n\n\nIn solving using this alternative approach we again see a MLE of 3.68.\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThis requires a slight modification to the previous function where the negative log-likelihood was calculated. Below the function is amended to take additional arguments in the form of beta values. These represent the various characteristics of a firm relative to their number of patents.\n\n\nCode\ndef neg_poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"Calculate the negative log-likelihood for the Poisson regression model.\"\"\"\n    linear_pred = np.dot(X, beta)\n    lambda_ = np.exp(linear_pred)\n    log_likelihood = np.sum(-lambda_ + Y * np.log(lambda_) - np.log(np.array([math.factorial(y) for y in Y])))\n    return -log_likelihood  # Return the negative log-likelihood for minimization\n\n\nProcessing of the data to create the regression includes a few steps. First, a column of ones is added to represent the intercept. Second, the square of ages is added as a new column. Third, categorical variables (region) are one-hot encoded to give them numerical values. Finally, these are joined together to create the new dataframe ‚ÄòX‚Äô and all values are converted to integers for ease of calculation. Once done the dataframes will be passed through to minimization function.\n\n\nCode\ndf['age_squared'] = df['age'] ** 2\nX = pd.concat([pd.DataFrame({'intercept': np.ones(len(df))}), df[['age', 'age_squared', 'iscustomer']], pd.get_dummies(df['region'], drop_first=True)], axis=1)\nY = df['patents']\n\n# Cast Y and Y as int\nY = Y.astype(int)\nX = X.astype(int)\n\ncolumn_names = X.columns\n\n\nThe output shows the feature name, the coefficient for each feature, and its standard error. You may notice that some of the standard errors are significantly larger than the coefficients.\n\n\nCode\nscaler = StandardScaler()\n\n# Check if X is a DataFrame and if it has more than one column\nif isinstance(X, pd.DataFrame) and X.shape[1] &gt; 1:\n    # Scale only the non-intercept columns if X is a DataFrame\n    X.iloc[:, 1:] = scaler.fit_transform(X.iloc[:, 1:])\nelse:\n    # If X is already a numpy array or has only one column, handle accordingly\n    print(\"Check the structure of X; it might not be a DataFrame or only contains one column.\")\n\n\n# Initial guess for the parameters (beta)\ninitial_beta = np.zeros(X.shape[1])\n\n# Use 'minimize' to find the MLE of beta\nresult = minimize(\n    neg_poisson_regression_loglikelihood, \n    x0=initial_beta, \n    args=(Y, X), \n    method='L-BFGS-B', \n    options={'disp': True, 'maxcor': 20, 'ftol': 1e-9, 'gtol': 1e-9}\n)\n# Check if the optimization was successful\nif result.success:\n    estimated_beta = result.x\n    print(\"Optimization successful.\")\nelse:\n    print(\"Optimization failed.\")\n    print(result.message)\n\nif 'hess_inv' in result:\n    # Convert the hess_inv to a numpy array if it isn't already\n    hess_inv_matrix = np.array(result.hess_inv.todense()) if not isinstance(result.hess_inv, np.ndarray) else result.hess_inv\n\n    # Compute standard errors\n    std_errors = np.sqrt(np.diag(hess_inv_matrix))\n\n# Create a DataFrame to display results nicely\nresults_df = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': estimated_beta,\n    'Standard Error': std_errors,\n}) \n\nprint(results_df)\n\n\nOptimization successful.\n      Variable  Coefficient  Standard Error\n0    intercept     1.286811        0.129237\n1          age     0.962663        1.281905\n2  age_squared    -1.055905        1.654322\n3   iscustomer     0.038395        0.735054\n4    Northeast     0.049702        0.404551\n5    Northwest    -0.006576        0.274687\n6        South     0.019809        0.817912\n7    Southwest     0.020025        0.653932\n\n\nLet‚Äôs check out results again using Python functions that already exist. To better visualize the potential effective sizes given the data, we‚Äôll plot the coefficients and their confidence intervals.\n\n\nCode\nX = np.asarray(X, dtype=np.int64)\nY = np.asarray(Y, dtype=np.int64)\n\n# Fit the GLM model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n\n\n\n\nCode\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 2)\n\n# Create a DataFrame to display results nicely\nresults_df_2 = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int_df = pd.DataFrame(conf_int, columns=['Lower CI', 'Upper CI'], index=column_names)\n\n# Merge with the coefficients data\nresults_viz_df = pd.concat([results_df_2.set_index('Variable'), conf_int_df], axis=1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_viz_df.index, data=results_viz_df, capsize=.2)\nfor i, (lower, upper) in enumerate(zip(results_viz_df['Lower CI'], results_viz_df['Upper CI'])):\n    plt.plot([lower, upper], [i, i], color='black')\nplt.title('Effect Sizes with 95% Confidence Intervals')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_2.to_string(index=False))\n\n\nModel Summary:\n\n\nCoefficient Estimates and Statistics:\n   Variable  Coefficient  Standard Error  P-Value\n  intercept     1.239070        0.036474     0.00\n        age     0.473599        0.109353     0.00\nage_squared    -0.544929        0.107140     0.00\n iscustomer     0.049033        0.019352     0.01\n  Northeast     0.111584        0.041997     0.01\n  Northwest    -0.003576        0.026911     0.89\n      South     0.025941        0.026343     0.32\n  Southwest     0.032560        0.023606     0.17\n\n\nTo determine whether or not being a customer is linked to higher numbers of patents we need to closely examine the final outputs. At first our exploratory data analysis showed there was a noticeable difference in mean number of patents between non-customers and customers.\nThe potential effect size as given by the data may not be as large. Our final model suggests a coefficient of 0.049 if ‚Äòiscustomer‚Äô is true. If we broaden our view of the data we see that the 95% confidence interval is a range of positive values and this makes sense when the standard error is 0.019.\nFrom this we can say that the marketing team is right, those using Blueprinty‚Äôs software are more successful in getting their patent applications approved. However, the level of success is potentially so small that one should consider whether the software‚Äôs use is truely a net benefit. The value of potential patents and cost of software implementation may make this a no-go for some potential users."
  },
  {
    "objectID": "projects/project2/index.html#airbnb-case-study",
    "href": "projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\nHere we will assume the number of reviews is a good proxy for the number of bookings. We‚Äôll explore the given data and see what effects the available features have on the number of reviews.\nTo start we‚Äôll load the data and perform some basic exploratory analysis.\n\n\nCode\n# Read in data\nairbnb_data = pd.read_csv('airbnb.csv')\nprint(airbnb_data.shape)\nprint(airbnb_data.head())\n\n\n(40628, 14)\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\nIdeally all of the data is a workable data type. Categorical variables should be one-hot encoded and any missing values should be removed.\n\n\nCode\nprint(airbnb_data.isnull().sum())\n\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\n\nCode\n# Convert dates\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Drop rows with any missing values in critical columns\nairbnb_data.dropna(subset=['host_since','bathrooms', 'bedrooms', 'price', 'number_of_reviews', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value'], inplace=True)\n\n# Convert 'instant_bookable' to a binary indicator\nairbnb_data['instant_bookable'] = (airbnb_data['instant_bookable'] == 't').astype(int)\n\nprint(airbnb_data.shape)\nprint(airbnb_data.isnull().sum())\n\n\n(30140, 14)\nUnnamed: 0                   0\nid                           0\ndays                         0\nlast_scraped                 0\nhost_since                   0\nroom_type                    0\nbathrooms                    0\nbedrooms                     0\nprice                        0\nnumber_of_reviews            0\nreview_scores_cleanliness    0\nreview_scores_location       0\nreview_scores_value          0\ninstant_bookable             0\ndtype: int64\n\n\n\n\nCode\n# Distribution of number of reviews\nsns.histplot(airbnb_data['number_of_reviews'], kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.show()\n\nprint(airbnb_data['number_of_reviews'].mean())\n\n\n\n\n\n\n\n\n\n21.168115461181156\n\n\nNow that the data has been cleaned and we have a general sense of the distribution we will attempt to model it using a Poisson Regression. This requires a few steps. First, the features and dependent variable should be split. Second, a constant is needed to service as the intercept. Third, we assure that all data is a common data type. Finally, we fit the model using built-in Python functions to replicate our Poisson regression.\n\n\nCode\n# Prepare the data for modeling\nX = airbnb_data[['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value', 'instant_bookable']]\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\ny = airbnb_data['number_of_reviews']\n\n# Adding constant for statsmodels\nX = sm.add_constant(X)\n\nX = X.astype(int)\ny = y.astype(int)\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 5)\n\n# Create a DataFrame to display results nicely\nresults_df_3 = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_df_3.index, data=results_df_3, capsize=.2)\nplt.title('Effect Sizes')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int.columns = ['Lower CI', 'Upper CI']\n\nresults_df_3 = pd.concat([results_df_3, conf_int], axis=1)\n\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_3.to_string(index=False))\n\n\nModel Summary:\n\n\nCoefficient Estimates and Statistics:\n                 Variable  Coefficient  Standard Error  P-Value  Lower CI  Upper CI\n                    const     2.931210        0.016672      0.0  2.898534  2.963887\n                     days     0.000522        0.000002      0.0  0.000518  0.000525\n                bathrooms    -0.095659        0.003917      0.0 -0.103336 -0.087982\n                 bedrooms     0.069663        0.002011      0.0  0.065723  0.073604\n                    price    -0.000053        0.000008      0.0 -0.000069 -0.000036\nreview_scores_cleanliness     0.110979        0.001517      0.0  0.108006  0.113952\n   review_scores_location    -0.081254        0.001617      0.0 -0.084424 -0.078084\n      review_scores_value    -0.091405        0.001847      0.0 -0.095025 -0.087786\n         instant_bookable     0.459400        0.002919      0.0  0.453678  0.465122\n   room_type_Private room     0.016086        0.002739      0.0  0.010719  0.021454\n    room_type_Shared room    -0.118490        0.008650      0.0 -0.135445 -0.101536\n\n\nThe plot above shows the general effect size for each of the features. By value the largest effects on the number of reviews are seen in the constant (intercept) with some additional effect by the ‚Äòinstant_bookable‚Äô feature. The table shows each variable (feature), it‚Äôs associated coefficient, standard error, and p-values as well as the upper and lower 95% confidence intervals.\nWhat we see here is statistically significant data, but with coefficients that are so narrowly bounded and close to zero that most of the features do not indicate an effect on the number of reviews. The coefficient of the intercept is very close to the log of the mean value of number of reviews. If the intercept value is close to the log of the mean of the dependent variable, this suggests that the model, without much contribution from other predictors, is essentially reverting to predicting the mean of the count data.\nThe approximate relationship can be expressed as \\(\\log(\\lambda) \\approx \\beta_0\\).\nWhen other predictors do not significantly influence the model, the intercept alone should closely estimate the log of the mean of the dependent variable."
  }
]