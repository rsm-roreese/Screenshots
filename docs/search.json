[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robin Reese",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "{{&lt; pdf files/resume.pdf width=100% height=800 &gt;}}"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nRobin Reese\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nRobin Reese\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Multi-nomial Logit (MNL) Model\n\n\n\n\n\n\nRobin Reese\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Importance\n\n\n\n\n\n\nRobin Reese\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn this field experiment involving over 50,000 prior donors to a non-profit, the researchers investigate how offering matching grants affects charitable giving. Two-thirds of the sample were placed into a treatment group, receiving solicitation letters highlighting that their donations would be matched by an anonymous donor at various ratios and maximum amounts, while the remaining third, serving as a control group, received standard solicitation materials without mention of a match. The treatment group was further stratified into subgroups, each receiving tailored messaging about the matching ratio, the cap on the matching grant, and suggested donation amounts based on their previous giving history.\nThe study‚Äôs key outcome measures were the response rate to the solicitation and the amount of money donated. By integrating subtle variations in the solicitation letter and the accompanying reply card, the experiment meticulously isolates the influence of the matching grant information. The results offer actionable insights into the effectiveness of different fundraising strategies, revealing the intricate ways in which donors‚Äô decisions are influenced not only by the economic benefit of matching grants but also by their previous engagement levels and the manner in which the opportunity to have their donation matched is communicated.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nWe‚Äôll start this investigation by loading the data and getting our various tools ready.\n\n\nCode\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in data\ndf = pd.read_csv('blueprinty.csv')\nprint(df.head())\n\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\nOne of the first items to check is the general distribution of patents held within the firm data.\n\n\nCode\n# Group the data by customer status\ngrouped = df.groupby('iscustomer')\n\n# Plot histograms and add the mean bar as a vertical line\ngrouped['patents'].plot(kind='hist', alpha=0.5, legend=True)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.show()\n\n# Calculate means\nmeans = grouped['patents'].mean()\nprint(means)\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nAt a glance, it does appear that those firms who are also customers of Blueprinty have a higher average number of patents. From the above we see that non-customers hold roughly 3.6 patents whereas customers hold 4.1 patents on average.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs plot the distribution of ages and location to see if anything stands out.\n\n\nCode\n# Create a new column for age bracket\ndf['agebracket'] = pd.cut(df['age'], bins=range(0, 51, 10), right=False)\n\n# Show the counts by customer status\nsns.countplot(x='agebracket', hue='iscustomer', data=df)\nplt.xlabel('Age Bracket')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Age Bracket')\nplt.show()\n\n# Print the mean customer status by age bracket\nprint(df.groupby('agebracket')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nagebracket\n[0, 10)     0.000000\n[10, 20)    0.219269\n[20, 30)    0.122708\n[30, 40)    0.083141\n[40, 50)    0.145455\nName: iscustomer, dtype: float64\n\n\n\n\nCode\n# Show the count of each region by customer status\nsns.countplot(x='region', hue='iscustomer', data=df)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Region')\nplt.show()\n\n# print the mean customer status by region\nprint(df.groupby('region')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nregion\nMidwest      0.075893\nNortheast    0.188020\nNorthwest    0.085561\nSouth        0.104712\nSouthwest    0.104377\nName: iscustomer, dtype: float64\n\n\nThere is definitely a skew in the data. Many firms are within the age range of 10 - 30 years. The region field also suggests that the firms that are also customers are heavily segmented in the Northeast region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nSimple Poison Model: \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nGiven the above, the expression\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\log \\left(\\prod_{i=1}^n y_i!\\right)\n\\]\nis the log-likelihood function for a set of ùëõ observations assumed to be independently and identically distributed according to a Poisson distribution with parameter Œª.\nIn Python that math would take the form of the following function:\n\n\nCode\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda values\n    # Calculate the log-likelihood\n    try:\n        log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([np.log(math.factorial(y)) for y in Y])\n    except OverflowError:  # Handling OverflowError that can occur with large factorials\n        log_likelihood = float('-inf')\n    return log_likelihood\n\n\nNow we can use that function to plot the log-likelihood against alues of lambda. The maximum likelihood estimator is shown at the peak of the curve. We can see that it takes the value of approximately 3.68 which you may remember is somewhat closer to the average number of patents of non-customers.\n\n\nCode\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, Y.max() + 2, 400)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]\n\n# Calculate Y_bar, the sample mean of the observed data Y\nY_bar = np.mean(Y)\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.axvline(x=Y_bar, color='red', linestyle='--', label=f'Y_bar at {Y_bar:.2f}', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda Values')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html#sub-header",
    "href": "projects/project2/index.html#sub-header",
    "title": "Test Page 2",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#introduction",
    "href": "projects/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn this field experiment involving over 50,000 prior donors to a non-profit, the researchers investigate how offering matching grants affects charitable giving. Two-thirds of the sample were placed into a treatment group, receiving solicitation letters highlighting that their donations would be matched by an anonymous donor at various ratios and maximum amounts, while the remaining third, serving as a control group, received standard solicitation materials without mention of a match. The treatment group was further stratified into subgroups, each receiving tailored messaging about the matching ratio, the cap on the matching grant, and suggested donation amounts based on their previous giving history.\nThe study‚Äôs key outcome measures were the response rate to the solicitation and the amount of money donated. By integrating subtle variations in the solicitation letter and the accompanying reply card, the experiment meticulously isolates the influence of the matching grant information. The results offer actionable insights into the effectiveness of different fundraising strategies, revealing the intricate ways in which donors‚Äô decisions are influenced not only by the economic benefit of matching grants but also by their previous engagement levels and the manner in which the opportunity to have their donation matched is communicated.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/index.html#data",
    "href": "projects/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe first step in our analysis of the data will be to load and explore the results. Below we bring in the state file and take a quick overview of the variables‚Äô counts, types, and distributions.\n\n\nCode\n# Read the .dta file\nimport pandas as pd\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# Get an overview of the data\ndf.info()\n\n# Describe the data\ndata_description = df.describe()\nprint(data_description)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168560      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378105     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258633  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n::::\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nDirect t-tests and t-tests from linear regression are both statistical tools used to assess the significance of differences between groups or the impact of predictors. A direct t-test compares the means of two independent samples to determine if they come from distributions with equal means, making it ideal for straightforward comparisons between two groups, such as control and treatment conditions in an experiment. The resulting p-value indicates whether any observed difference is likely to be due to chance.\nIn contrast, a t-test from a linear regression analysis evaluates the significance of individual predictors within a more complex model that may include multiple variables. The t-statistic here assesses whether a coefficient differs significantly from zero, taking into account other factors in the model. This allows for the evaluation of each predictor‚Äôs unique contribution and the control of confounding variables. Both types of t-tests rely on the assumption of normally distributed errors and can be adapted for equal or unequal variances between groups. While direct t-tests are best suited for simpler experimental designs, regression t-tests excel in multifaceted studies where multiple influences need to be accounted for simultaneously.\nWe‚Äôll define functions next that are able to perform both test methodologies on our dataset as needed.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef direct_t_test(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data for consistency with regression\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Separate the treatment and control groups\n    treatment_group = df[df[treatment_col] == 1][outcome_col]\n    control_group = df[df[treatment_col] == 0][outcome_col]\n\n    # Calculate means\n    mean_treatment = treatment_group.mean()\n    mean_control = control_group.mean()\n\n    # Calculate standard deviations\n    std_treatment = treatment_group.std(ddof=1)\n    std_control = control_group.std(ddof=1)\n\n    # Calculate sample sizes\n    n_treatment = len(treatment_group)\n    n_control = len(control_group)\n\n    # Calculate separate standard errors\n    se_treatment = std_treatment / np.sqrt(n_treatment)\n    se_control = std_control / np.sqrt(n_control)\n\n    # Calculate the t-statistic\n    t_stat = (mean_treatment - mean_control) / np.sqrt(se_treatment**2 + se_control**2)\n\n    # Calculate degrees of freedom using the Welch-Satterthwaite equation\n    df = ((se_treatment**2 + se_control**2)**2 /\n          ((se_treatment**4 / (n_treatment - 1)) + (se_control**4 / (n_control - 1))))\n\n    return t_stat, df\n\ndef run_regression(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Prepare the design matrix X with a constant (intercept) and the treatment indicator\n    X = sm.add_constant(df[treatment_col])\n    Y = df[outcome_col]\n\n    # Fit the OLS regression model\n    model = sm.OLS(Y, X).fit()\n\n    # Extract the t-statistic and p-value for the treatment variable\n    t_stat = model.tvalues[treatment_col]\n\n    # Return the t-statistic and degrees of freedom\n    return t_stat, model.df_resid\n\n# Example usage (you need to replace 'your_dataframe', 'treatment', and 'outcome' with your actual DataFrame and column names)\n# t_stat_direct, df_direct = direct_t_test(your_dataframe, 'treatment', 'outcome')\n# t_stat_regression, df_regression = run_regression(your_dataframe, 'treatment', 'outcome')\n# print(\"Direct t-test:\", t_stat_direct, \"df:\", df_direct)\n# print(\"Regression t-test:\", t_stat_regression, \"df:\", df_regression)\n\n\nNow that the functions are defined, we can test different variables besides the outcome variables to see if the treatment and control groups show significant differences from each other.\n\n\nCode\n# Prepare groups for direct t-test of 'mrm2'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'mrm2')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'mrm2')\nprint('Testing the mrm2 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'female'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'female')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'female')\nprint('Testing the female variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'red0'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'red0')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'red0')\nprint('Testing the red0 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'years')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'years')\nprint('Testing the years variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n\nTesting the mrm2 variable\nDirect t-test: 0.11953155228176904\nRegression t-test: 0.11949210581591684\nTesting the female variable\nDirect t-test: -1.7535132542518672\nRegression t-test: -1.7583691871450848\nTesting the red0 variable\nDirect t-test: 1.877281316440582\nRegression t-test: 1.8750884882812506\nTesting the years variable\nDirect t-test: -1.0909175279573782\nRegression t-test: -1.1030383745788988"
  },
  {
    "objectID": "projects/project1/index.html#experimental-results",
    "href": "projects/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nAnalyzing whether matched donations lead to an increased response rate of making a donation is a critical first step in reproducing the results of a study because it directly assesses the effectiveness of the matching incentive as a motivational tool in charitable giving. This analysis establishes the foundational evidence needed to validate the underlying assumptions of fundraising strategies that utilize matching offers to enhance donor engagement and generosity. Below we‚Äôll plot the differences in response rate between the treatment and control groups.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Calculate the proportions for the 'gave' column based on the 'treatment' indicator\nproportions = df.groupby('treatment')['gave'].mean()\n\n# Create a barplot\nplt.figure(figsize=(10, 6))\ncolors = ['#1f77b4', '#ff7f0e']  # Aesthetically pleasing color palette\nproportions.plot(kind='bar', color=colors)\n\n# Add labels and title\nplt.title('Proportion of People Who Donated by Group')\nplt.xlabel('Group')\nplt.ylabel('Proportion of Donations')\nplt.xticks(ticks=[0, 1], labels=['Control', 'Treatment'], rotation=0)  # Rename x-ticks for clarity\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nIt appears from a visualization that there is a difference. The treatment group has a slightly higher response rate than does the control group. How sure are we that this effect size is statistically significant rather than due to random chance?\nTo determine this the next step is to run a t-test between the treatment and control groups on the donation outcome. Using the binary variable ‚Äògave‚Äô. This designates whether a person gave a donation in any amount.\n\n\nCode\nfrom scipy import stats\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'gave')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'gave')\nprint('Testing the gave variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n\n\nTesting the gave variable\nDirect t-test: 3.2094621908279835\nRegression t-test: 3.101361000543946\nP-value: 0.001330982345091547\n\n\nThe above results show stastical values that lead one to believe the difference in response rates between the treatment and control groups are unlikely to have occurred by chance. It indicates that such financial incentives can effectively influence human behavior, enhancing the likelihood of donating or possibly increasing the donation amounts.\n\n\nCode\nimport statsmodels.formula.api as smf\n\n# Run the probit regression model\ndef run_probit_regression(df, formula):\n    # Probit model using the formula interface\n    model = smf.probit(formula, data=df)\n    results = model.fit(disp=0)\n    return results\n\n# Use the probit regression function\nformula = 'gave ~ treatment'\nprobit_results = run_probit_regression(df, formula)\n\n# Print out the summary of the regression results\ncoefficients = probit_results.params\np_values = probit_results.pvalues\nprint(\"Treatment Coefficient: \", coefficients[1])\nprint(\"Treatment P-Value: \", p_values[1])\n\n\nTreatment Coefficient:  0.08678462244745781\nTreatment P-Value:  0.0018523990147786633\n\n\n\n\nDifferences between Match Rates\nIn the realm of charitable giving, the size of matched donations often plays a pivotal role in incentivizing potential donors. Understanding how different matching ratios influence donor behavior can provide valuable insights for optimizing fundraising strategies. This analysis focuses on evaluating the impact of various match rates‚Äî1:1, 2:1, and 3:1‚Äîon the likelihood of donations. By employing a series of t-tests, the study seeks to determine if higher match ratios significantly increase the response rate among donors, thereby testing the assumption that more generous matching offers might lead to higher participation rates in donation campaigns.\nBelow is the code to assess the differences in rates of response between different matching offers presented:\n\n\nCode\nfrom scipy import stats\n\n# Calculate the mean donation rate for each ratio category\nmean_ratio1 = df[df['ratio'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Conduct t-tests\nt_test_1_vs_2 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio2'] == 1]['gave'], equal_var=False)\nt_test_1_vs_3 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\nt_test_2_vs_3 = stats.ttest_ind(df[df['ratio2'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\n\n# Print the results\nprint(f\"Mean Donation Rate for 1:1 match: {mean_ratio1:.4f}\")\nprint(f\"Mean Donation Rate for 2:1 match: {mean_ratio2:.4f}\")\nprint(f\"Mean Donation Rate for 3:1 match: {mean_ratio3:.4f}\")\nprint(\"T-test Results:\")\nprint(\"1:1 vs 2:1:\", t_test_1_vs_2)\nprint(\"1:1 vs 3:1:\", t_test_1_vs_3)\nprint(\"2:1 vs 3:1:\", t_test_2_vs_3)\n\n\nMean Donation Rate for 1:1 match: 0.0207\nMean Donation Rate for 2:1 match: 0.0226\nMean Donation Rate for 3:1 match: 0.0227\nT-test Results:\n1:1 vs 2:1: TtestResult(statistic=-0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836)\n1:1 vs 3:1: TtestResult(statistic=-1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684)\n2:1 vs 3:1: TtestResult(statistic=-0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778)\n\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors‚Äô comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\nNext we‚Äôll use regression to look at the comparison of mean response rates for different matching ratios on whether people chose to donate. The coefficient of each variable [ratio1, ratio2, ratio3] can be looked at to determine the effect size between levels of matching presented. The analysis gives us a p-value as well to determine the significance of the movement suggested by the coefficient.\n\n\nCode\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n# Prepare the design matrix X with a constant and the dummy variables for ratio\nX = sm.add_constant(df[['ratio1', 'ratio2', 'ratio3']])\nY = df['gave']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n\n\n{'const': {'Coefficient': 0.01785821298016419, 'P-value': 4.7869277141173614e-59}, 'ratio1': {'Coefficient': 0.002890911245112018, 'P-value': 0.09662209260247782}, 'ratio2': {'Coefficient': 0.004775162266827, 'P-value': 0.006062639303220395}, 'ratio3': {'Coefficient': 0.004875186247079949, 'P-value': 0.005086912091717824}}\n\n\nInteresting. The summary output will be returned to shortly. Next we‚Äôll compare the mean response rates between the matching offers. This will show how much the response rate moves as the offer is increased from 1:1 to 2:1 to 3:1.\n\n\nCode\nmean_ratio1 = df[df['ratio1'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Calculate the differences directly from the data\ndifference_1_to_2 = mean_ratio2 - mean_ratio1\ndifference_2_to_3 = mean_ratio3 - mean_ratio2\n\nprint(f\"Directly calculated difference in response rate from 1:1 to 2:1 match ratio: {difference_1_to_2:.4f}\")\nprint(f\"Directly calculated difference in response rate from 2:1 to 3:1 match ratio: {difference_2_to_3:.4f}\")\n\n\nDirectly calculated difference in response rate from 1:1 to 2:1 match ratio: 0.0019\nDirectly calculated difference in response rate from 2:1 to 3:1 match ratio: 0.0001\n\n\nFor housekeeping we should look to see if the model coefficients show a similar movement to the mean comparisons.\n\n\nCode\ncoefficients = model.params\n\n# Calculate the difference between ratio2 - ratio1 and ratio3 - ratio2\ndifference_ratio2_ratio1 = coefficients['ratio2'] - coefficients['ratio1']\ndifference_ratio3_ratio2 = coefficients['ratio3'] - coefficients['ratio2']\n\ndifference_ratio2_ratio1, difference_ratio3_ratio2\n\nprint(f\"Coefficient comparison difference in response rate from 1:1 to 2:1 match ratio: {difference_ratio2_ratio1:.4f}\")\nprint(f\"Coefficient comparison difference in response rate from 2:1 to 3:1 match ratio: {difference_ratio3_ratio2:.4f}\")\n\n\nCoefficient comparison difference in response rate from 1:1 to 2:1 match ratio: 0.0019\nCoefficient comparison difference in response rate from 2:1 to 3:1 match ratio: 0.0001\n\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors‚Äô comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution. In order to do this we shall run the same t-test only this time we‚Äôre using the donation amount relative to the treatment effect.\n\n\nCode\n# Perform both t-tests of 'amount'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'amount')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'amount')\nprint('Testing the amount variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n\n\nTesting the amount variable\nDirect t-test: 1.9182283233541\nRegression t-test: 1.860502691500859\nP-value: 0.0550899208482003\n\n\nThe analysis suggests that matched donations have a potentially positive, though not statistically significant, impact on donation amounts at the traditional 5% significance level. Given the proximity of the p-value to this threshold, organizations might still consider matched donations as part of a broader, diversified fundraising strategy. Barring additional research, it may yet prove a positive influence on donation amounts.\nAs a next step we‚Äôll filter the data to only show those who donated something. This can then be compared to the treatment variable to see if treatment has an effect on the amount given that can be teased out of the data.\n\n\nCode\n# Filter to include only rows where a positive donation was made\ndf_donors = df[df['amount'] &gt; 0]\n\n# Prepare the design matrix X with a constant and the treatment indicator\nX = sm.add_constant(df_donors['treatment'])\nY = df_donors['amount']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n\n\n{'const': {'Coefficient': 45.54026845637584, 'P-value': 5.473577513353547e-68}, 'treatment': {'Coefficient': -1.6683934553392588, 'P-value': 0.5614755766155095}}\n\n\nThe coefficient for the treatment variable, which is negative, suggests that being in the treatment group (i.e., offered a matched donation) is associated with a decrease in the donation amount by approximately $1.67 compared to the control group. However, the p-value associated with this coefficient is 0.561, indicating that this effect is not statistically significant.\nThe regression analysis reveals that, contrary to expectations, the treatment (matched donations) does not lead to an increase in the amount donated. Instead, there‚Äôs an indication (though not statistically significant) that it might decrease the amount donated if we are to read the outputs of the regression. All in all, there is not enough information here to say that donation amount specifically is moved by the treatment.\n\n\nCode\ntreatment_donated = df_donors[df_donors['treatment'] == 1]\ncontrol_donated = df_donors[df_donors['treatment'] == 0]\n\n# Calculate the mean donation amount for each group\nmean_treatment = treatment_donated['amount'].mean()\nmean_control = control_donated['amount'].mean()\n\n# Creating histograms\nplt.figure(figsize=(14, 6))\n\n# Histogram for treatment group\nplt.subplot(1, 2, 1)\nplt.hist(treatment_donated['amount'], bins=30, color='blue', alpha=0.7)\nplt.axvline(mean_treatment, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Treatment Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_treatment:.2f}', xy=(mean_treatment, 50),\n             xytext=(mean_treatment + 50, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='right')\n\n# Histogram for control group\nplt.subplot(1, 2, 2)\nplt.hist(control_donated['amount'], bins=30, color='green', alpha=0.7)\nplt.axvline(mean_control, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Control Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_control:.2f}', xy=(mean_control, 20),\n             xytext=(mean_control - 150, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='left')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/project1/index.html#simulation-experiment",
    "href": "projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nIn the provided text, the author outlines an educational simulation designed to illustrate key statistical principles, namely the Law of Large Numbers and the Central Limit Theorem, using a practical example from a charitable donation context. By setting up a scenario where the probabilities of making a donation differ between respondents who receive a match and those who do not, the simulation aims to show how differences in probabilities influence donation behaviors over a large number of trials. Specifically, respondents who do not receive a match have a lower probability (0.018) of donating compared to those who receive a match (0.022).\nThe code below is structured to generate a substantial number of simulations‚Äî10,000 for each group‚Äîto model donation outcomes according to the specified Bernoulli distributions. It uses Python‚Äôs numpy library to simulate these outcomes, ensuring reproducibility by setting a random seed. Once the donation data for both control (no match) and treatment (match) groups are simulated, the script calculates the cumulative average of the differences in donation probabilities between the two groups across the number of draws. This method allows the plot to visually depict how the average differences evolve as more data points are considered, highlighting the convergence behavior predicted by the Law of Large Numbers.\n\n\nCode\n# Set the true probabilities for control and treatment\ntrue_prob_control = 0.018\ntrue_prob_treatment = 0.022\n\n# Number of simulations/draws\nnum_simulations = 10000\n\n# Simulate donations for control and treatment groups\nnp.random.seed(0)  # For reproducibility\ncontrol_donations = np.random.binomial(1, true_prob_control, num_simulations)\ntreatment_donations = np.random.binomial(1, true_prob_treatment, num_simulations)\n\n# Compute the cumulative average of the differences\ncumulative_differences = np.cumsum(treatment_donations - control_donations) / np.arange(1, num_simulations + 1)\n\n# Plot the cumulative averages\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_differences, color='red', lw=2)\nplt.axhline(y=true_prob_treatment - true_prob_control, color='blue', lw=1, linestyle='--')\n\nplt.xlabel('Number of Draws')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Cumulative Average Difference in Donation Rate')\nplt.show()\n\nprint(\"The mean difference is \" + str(true_prob_treatment - true_prob_control) + \" (the blue hash line).\")\n\n\n\n\n\n\n\n\n\nThe mean difference is 0.004 (the blue hash line).\n\n\nThe plot generated by the code visually confirms the theoretical expectation: as the number of simulations increases, the cumulative average of the differences should approach the true difference in means (0.004) between the control and treatment groups. This is depicted on the plot by the blue dashed line at the level of the true mean difference. The exercise not only reinforces the statistical theory behind sampling distributions and their averages but also provides an intuitive grasp of how small probability differences can be detected and quantified with sufficient data, underlining the practical applications of these concepts in analyzing and interpreting data from real-world experiments. This simulation thereby serves as a powerful tool in both teaching and understanding statistical inference through a direct and engaging approach.\n\n\nCentral Limit Theorem\nThe Central Limit Theorem (CLT) is a fundamental principle in statistics that plays a pivotal role in the simulation above. It states that, regardless of the distribution of the population, the distribution of the sample means will approximate a normal distribution as the sample size increases, provided the samples are independent and identically distributed with a finite mean and variance. This theorem is crucial because it justifies the use of normal probability theory in the inference about the mean of a population, even when the population itself is not normally distributed.\nIn this scenario, the Central Limit Theorem comes into play by ensuring that the distribution of the cumulative average differences between the treatment and control groups will approach a normal distribution as the number of draws (i.e., sample size) increases. Each draw represents a Bernoulli trial where a donation is made with a certain probability. As you simulate more trials, the average of these results (due to the law of large numbers) will converge not only towards the true mean difference but also the distribution of these averages will start to resemble a normal distribution (thanks to the CLT).\n\n\nCode\n# Let's generate the histograms for the sample sizes specified and comment on their distribution.\n\n# Function to simulate the process and calculate the averages\ndef simulate_averages(sample_size, repetitions, p_control, p_treatment):\n    control_means = np.random.binomial(sample_size, p_control, repetitions) / sample_size\n    treatment_means = np.random.binomial(sample_size, p_treatment, repetitions) / sample_size\n    return treatment_means - control_means\n\n# Sample sizes to generate histograms for\nsample_sizes = [50, 200, 500, 1000]\n\n# Control and treatment probabilities\np_control = 0.018\np_treatment = 0.022\n\n# Number of repetitions to calculate averages\nrepetitions = 1000\n\n# Set up the plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# Generate the histograms\nfor i, sample_size in enumerate(sample_sizes):\n    # Simulate the differences in averages\n    avg_differences = simulate_averages(sample_size, repetitions, p_control, p_treatment)\n    \n    # Plot histogram horizontally\n    axes[i].hist(avg_differences, bins=30, orientation='horizontal', color='skyblue', edgecolor='black')\n    mean_value = np.mean(avg_differences)\n    axes[i].axhline(y=mean_value, color='red', linestyle='dashed', linewidth=2)\n    \n    # Annotate the mean difference\n    axes[i].annotate(f'Mean: {mean_value:.4f}', xy=(0.5, mean_value), xytext=(10, 0), \n                     textcoords='offset points', fontsize=10, color='red', ha='center', va='bottom')\n    \n    # Set title and labels\n    axes[i].set_title(f'Sample size: {sample_size}')\n    axes[i].set_ylabel('Average Difference')\n    axes[i].set_xlabel('Frequency')\n\n# Adjust the layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFor each histogram corresponding to different sample sizes (50, 200, 500, 1000), zero should ideally be within the tails of the distribution if there is no true difference.\nSince the true probabilities differ by 0.004 (p_treatment - p_control), zero is not expected to be in the center if the simulation reflects the true difference.\nAs sample size increases, the distribution of the average differences should become more centered around the true difference (0.004), and the variance should decrease, making the distribution narrower around the mean. Though there is an element of randomness in each sample, we can see that as n increases the peak of the histogram starts to converge around the mean value we know to be 0.004."
  },
  {
    "objectID": "projects/project2/index.html#blueprinty-case-study",
    "href": "projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nWe‚Äôll start this investigation by loading the data and getting our various tools ready.\n\n\nCode\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in data\ndf = pd.read_csv('blueprinty.csv')\nprint(df.head())\n\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\nOne of the first items to check is the general distribution of patents held within the firm data.\n\n\nCode\n# Group the data by customer status\ngrouped = df.groupby('iscustomer')\n\n# Plot histograms and add the mean bar as a vertical line\ngrouped['patents'].plot(kind='hist', alpha=0.5, legend=True)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.show()\n\n# Calculate means\nmeans = grouped['patents'].mean()\nprint(means)\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.623177\n1    4.091371\nName: patents, dtype: float64\n\n\nAt a glance, it does appear that those firms who are also customers of Blueprinty have a higher average number of patents. From the above we see that non-customers hold roughly 3.6 patents whereas customers hold 4.1 patents on average.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs plot the distribution of ages and location to see if anything stands out.\n\n\nCode\n# Create a new column for age bracket\ndf['agebracket'] = pd.cut(df['age'], bins=range(0, 51, 10), right=False)\n\n# Show the counts by customer status\nsns.countplot(x='agebracket', hue='iscustomer', data=df)\nplt.xlabel('Age Bracket')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Age Bracket')\nplt.show()\n\n# Print the mean customer status by age bracket\nprint(df.groupby('agebracket')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nagebracket\n[0, 10)     0.000000\n[10, 20)    0.219269\n[20, 30)    0.122708\n[30, 40)    0.083141\n[40, 50)    0.145455\nName: iscustomer, dtype: float64\n\n\n\n\nCode\n# Show the count of each region by customer status\nsns.countplot(x='region', hue='iscustomer', data=df)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Region')\nplt.show()\n\n# print the mean customer status by region\nprint(df.groupby('region')['iscustomer'].mean())\n\n\n\n\n\n\n\n\n\nregion\nMidwest      0.075893\nNortheast    0.188020\nNorthwest    0.085561\nSouth        0.104712\nSouthwest    0.104377\nName: iscustomer, dtype: float64\n\n\nThere is definitely a skew in the data. Many firms are within the age range of 10 - 30 years. The region field also suggests that the firms that are also customers are heavily segmented in the Northeast region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nSimple Poison Model: \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nGiven the above, the expression\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\log \\left(\\prod_{i=1}^n y_i!\\right)\n\\]\nis the log-likelihood function for a set of ùëõ observations assumed to be independently and identically distributed according to a Poisson distribution with parameter Œª.\nIn Python that math would take the form of the following function:\n\n\nCode\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda values\n    # Calculate the log-likelihood\n    try:\n        log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([np.log(math.factorial(y)) for y in Y])\n    except OverflowError:  # Handling OverflowError that can occur with large factorials\n        log_likelihood = float('-inf')\n    return log_likelihood\n\n\nNow we can use that function to plot the log-likelihood against alues of lambda. The maximum likelihood estimator is shown at the peak of the curve. We can see that it takes the value of approximately 3.68 which you may remember is somewhat closer to the average number of patents of non-customers.\n\n\nCode\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, Y.max() + 2, 400)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]\n\n# Calculate Y_bar, the sample mean of the observed data Y\nY_bar = np.mean(Y)\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.axvline(x=Y_bar, color='red', linestyle='--', label=f'Y_bar at {Y_bar:.2f}', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda Values')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html#mathematical-derivation",
    "href": "projects/project2/index.html#mathematical-derivation",
    "title": "Poisson Regression Examples",
    "section": "Mathematical Derivation",
    "text": "Mathematical Derivation\nFor a set of observations \\(( Y = y_1, y_2, \\ldots, y_n )\\) that are independently and identically distributed according to a Poisson distribution, the log-likelihood function \\(\\lambda\\) is given by:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right)\n\\]\nTaking the derivative with respect to \\(\\lambda\\) and setting it to zero:\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( \\frac{y_i}{\\lambda} - 1 \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n\\]\nSetting this derivative to zero for maximization:\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i = n\n\\]\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{Y}\n\\]\nThis results in \\(\\lambda\\) being equal to \\(\\bar{Y}\\), which confirms that the mean of the Poisson distribution \\(\\lambda\\) is also the rate parameter that maximizes the likelihood of observing the given data (i.e.¬†the top of the curve on our plot).\nWe can also find this peak value by inputting this function into Python‚Äôs minimize function. Keep in mind that we are trying to maximize the function so instead we‚Äôll pass the negative log-likelihood.\n\n\nCode\n# Define the negative log-likelihood function\ndef neg_poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the negative log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_[0] &lt;= 0:\n        return np.inf  # Return infinity if lambda is not positive\n    return -np.sum(Y * np.log(lambda_[0]) - lambda_[0] - gammaln(Y + 1))\n\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Initial guess for lambda\ninitial_lambda = np.array([4.5]) \n\n# Using 'minimize' from scipy.optimize to find the MLE for lambda\nresult = minimize(neg_poisson_loglikelihood, x0=initial_lambda, args=(Y,), bounds=[(0.001, None)])\n\n# The optimal lambda found\nlambda_mle = result.x[0]\n\nprint(\"The MLE for lambda is:\", lambda_mle)\n\n\nThe MLE for lambda is: 3.68466651967392\n\n\nIn solving using this alternative approach we again see a MLE of 3.68.\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThis requires a slight modification to the previous function where the negative log-likelihood was calculated. Below the function is amended to take additional arguments in the form of beta values. These represent the various characteristics of a firm relative to their number of patents.\n\n\nCode\ndef neg_poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"Calculate the negative log-likelihood for the Poisson regression model.\"\"\"\n    linear_pred = np.dot(X, beta)\n    lambda_ = np.exp(linear_pred)\n    log_likelihood = np.sum(-lambda_ + Y * np.log(lambda_) - np.log(np.array([math.factorial(y) for y in Y])))\n    return -log_likelihood  # Return the negative log-likelihood for minimization\n\n\nProcessing of the data to create the regression includes a few steps. First, a column of ones is added to represent the intercept. Second, the square of ages is added as a new column. Third, categorical variables (region) are one-hot encoded to give them numerical values. Finally, these are joined together to create the new dataframe ‚ÄòX‚Äô and all values are converted to integers for ease of calculation. Once done the dataframes will be passed through to minimization function.\n\n\nCode\ndf['age_squared'] = df['age'] ** 2\nX = pd.concat([pd.DataFrame({'intercept': np.ones(len(df))}), df[['age', 'age_squared', 'iscustomer']], pd.get_dummies(df['region'], drop_first=True)], axis=1)\nY = df['patents']\n\n# Cast Y and Y as int\nY = Y.astype(int)\nX = X.astype(int)\n\ncolumn_names = X.columns\n\n\nThe output shows the feature name, the coefficient for each feature, and its standard error. You may notice that some of the standard errors are significantly larger than the coefficients.\n\n\nCode\nscaler = StandardScaler()\n\n# Check if X is a DataFrame and if it has more than one column\nif isinstance(X, pd.DataFrame) and X.shape[1] &gt; 1:\n    # Scale only the non-intercept columns if X is a DataFrame\n    X.iloc[:, 1:] = scaler.fit_transform(X.iloc[:, 1:])\nelse:\n    # If X is already a numpy array or has only one column, handle accordingly\n    print(\"Check the structure of X; it might not be a DataFrame or only contains one column.\")\n\n\n# Initial guess for the parameters (beta)\ninitial_beta = np.zeros(X.shape[1])\n\n# Use 'minimize' to find the MLE of beta\nresult = minimize(\n    neg_poisson_regression_loglikelihood, \n    x0=initial_beta, \n    args=(Y, X), \n    method='L-BFGS-B', \n    options={'disp': True, 'maxcor': 20, 'ftol': 1e-9, 'gtol': 1e-9}\n)\n# Check if the optimization was successful\nif result.success:\n    estimated_beta = result.x\n    print(\"Optimization successful.\")\nelse:\n    print(\"Optimization failed.\")\n    print(result.message)\n\nif 'hess_inv' in result:\n    # Convert the hess_inv to a numpy array if it isn't already\n    hess_inv_matrix = np.array(result.hess_inv.todense()) if not isinstance(result.hess_inv, np.ndarray) else result.hess_inv\n\n    # Compute standard errors\n    std_errors = np.sqrt(np.diag(hess_inv_matrix))\n\n# Create a DataFrame to display results nicely\nresults_df = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': estimated_beta,\n    'Standard Error': std_errors,\n}) \n\nprint(results_df)\n\n\nOptimization successful.\n      Variable  Coefficient  Standard Error\n0    intercept     1.286811        0.129237\n1          age     0.962663        1.281905\n2  age_squared    -1.055905        1.654322\n3   iscustomer     0.038395        0.735054\n4    Northeast     0.049702        0.404551\n5    Northwest    -0.006576        0.274687\n6        South     0.019809        0.817912\n7    Southwest     0.020025        0.653932\n\n\nLet‚Äôs check out results again using Python functions that already exist. To better visualize the potential effective sizes given the data, we‚Äôll plot the coefficients and their confidence intervals.\n\n\nCode\nX = np.asarray(X, dtype=np.int64)\nY = np.asarray(Y, dtype=np.int64)\n\n# Fit the GLM model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n\n\n\n\nCode\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 2)\n\n# Create a DataFrame to display results nicely\nresults_df_2 = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int_df = pd.DataFrame(conf_int, columns=['Lower CI', 'Upper CI'], index=column_names)\n\n# Merge with the coefficients data\nresults_viz_df = pd.concat([results_df_2.set_index('Variable'), conf_int_df], axis=1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_viz_df.index, data=results_viz_df, capsize=.2)\nfor i, (lower, upper) in enumerate(zip(results_viz_df['Lower CI'], results_viz_df['Upper CI'])):\n    plt.plot([lower, upper], [i, i], color='black')\nplt.title('Effect Sizes with 95% Confidence Intervals')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_2.to_string(index=False))\n\n\nModel Summary:\n\n\nCoefficient Estimates and Statistics:\n   Variable  Coefficient  Standard Error  P-Value\n  intercept     1.239070        0.036474     0.00\n        age     0.473599        0.109353     0.00\nage_squared    -0.544929        0.107140     0.00\n iscustomer     0.049033        0.019352     0.01\n  Northeast     0.111584        0.041997     0.01\n  Northwest    -0.003576        0.026911     0.89\n      South     0.025941        0.026343     0.32\n  Southwest     0.032560        0.023606     0.17\n\n\nTo determine whether or not being a customer is linked to higher numbers of patents we need to closely examine the final outputs. At first our exploratory data analysis showed there was a noticeable difference in mean number of patents between non-customers and customers.\nThe potential effect size as given by the data may not be as large. Our final model suggests a coefficient of 0.049 if ‚Äòiscustomer‚Äô is true. If we broaden our view of the data we see that the 95% confidence interval is a range of positive values and this makes sense when the standard error is 0.019.\nFrom this we can say that the marketing team is right, those using Blueprinty‚Äôs software are more successful in getting their patent applications approved. However, the level of success is potentially so small that one should consider whether the software‚Äôs use is truely a net benefit. The value of potential patents and cost of software implementation may make this a no-go for some potential users."
  },
  {
    "objectID": "projects/project2/index.html#airbnb-case-study",
    "href": "projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\nHere we will assume the number of reviews is a good proxy for the number of bookings. We‚Äôll explore the given data and see what effects the available features have on the number of reviews.\nTo start we‚Äôll load the data and perform some basic exploratory analysis.\n\n\nCode\n# Read in data\nairbnb_data = pd.read_csv('airbnb.csv')\nprint(airbnb_data.shape)\nprint(airbnb_data.head())\n\n\n(40628, 14)\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\nIdeally all of the data is a workable data type. Categorical variables should be one-hot encoded and any missing values should be removed.\n\n\nCode\nprint(airbnb_data.isnull().sum())\n\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\n\nCode\n# Convert dates\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Drop rows with any missing values in critical columns\nairbnb_data.dropna(subset=['host_since','bathrooms', 'bedrooms', 'price', 'number_of_reviews', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value'], inplace=True)\n\n# Convert 'instant_bookable' to a binary indicator\nairbnb_data['instant_bookable'] = (airbnb_data['instant_bookable'] == 't').astype(int)\n\nprint(airbnb_data.shape)\nprint(airbnb_data.isnull().sum())\n\n\n(30140, 14)\nUnnamed: 0                   0\nid                           0\ndays                         0\nlast_scraped                 0\nhost_since                   0\nroom_type                    0\nbathrooms                    0\nbedrooms                     0\nprice                        0\nnumber_of_reviews            0\nreview_scores_cleanliness    0\nreview_scores_location       0\nreview_scores_value          0\ninstant_bookable             0\ndtype: int64\n\n\n\n\nCode\n# Distribution of number of reviews\nsns.histplot(airbnb_data['number_of_reviews'], kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.show()\n\nprint(airbnb_data['number_of_reviews'].mean())\n\n\n\n\n\n\n\n\n\n21.168115461181156\n\n\nNow that the data has been cleaned and we have a general sense of the distribution we will attempt to model it using a Poisson Regression. This requires a few steps. First, the features and dependent variable should be split. Second, a constant is needed to service as the intercept. Third, we assure that all data is a common data type. Finally, we fit the model using built-in Python functions to replicate our Poisson regression.\n\n\nCode\n# Prepare the data for modeling\nX = airbnb_data[['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value', 'instant_bookable']]\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\ny = airbnb_data['number_of_reviews']\n\n# Adding constant for statsmodels\nX = sm.add_constant(X)\n\nX = X.astype(int)\ny = y.astype(int)\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 5)\n\n# Create a DataFrame to display results nicely\nresults_df_3 = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_df_3.index, data=results_df_3, capsize=.2)\nplt.title('Effect Sizes')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int.columns = ['Lower CI', 'Upper CI']\n\nresults_df_3 = pd.concat([results_df_3, conf_int], axis=1)\n\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_3.to_string(index=False))\n\n\nModel Summary:\n\n\nCoefficient Estimates and Statistics:\n                 Variable  Coefficient  Standard Error  P-Value  Lower CI  Upper CI\n                    const     2.931210        0.016672      0.0  2.898534  2.963887\n                     days     0.000522        0.000002      0.0  0.000518  0.000525\n                bathrooms    -0.095659        0.003917      0.0 -0.103336 -0.087982\n                 bedrooms     0.069663        0.002011      0.0  0.065723  0.073604\n                    price    -0.000053        0.000008      0.0 -0.000069 -0.000036\nreview_scores_cleanliness     0.110979        0.001517      0.0  0.108006  0.113952\n   review_scores_location    -0.081254        0.001617      0.0 -0.084424 -0.078084\n      review_scores_value    -0.091405        0.001847      0.0 -0.095025 -0.087786\n         instant_bookable     0.459400        0.002919      0.0  0.453678  0.465122\n   room_type_Private room     0.016086        0.002739      0.0  0.010719  0.021454\n    room_type_Shared room    -0.118490        0.008650      0.0 -0.135445 -0.101536\n\n\nThe plot above shows the general effect size for each of the features. By value the largest effects on the number of reviews are seen in the constant (intercept) with some additional effect by the ‚Äòinstant_bookable‚Äô feature. The table shows each variable (feature), it‚Äôs associated coefficient, standard error, and p-values as well as the upper and lower 95% confidence intervals.\nWhat we see here is statistically significant data, but with coefficients that are so narrowly bounded and close to zero that most of the features do not indicate an effect on the number of reviews. The coefficient of the intercept is very close to the log of the mean value of number of reviews. If the intercept value is close to the log of the mean of the dependent variable, this suggests that the model, without much contribution from other predictors, is essentially reverting to predicting the mean of the count data.\nThe approximate relationship can be expressed as \\(\\log(\\lambda) \\approx \\beta_0\\).\nWhen other predictors do not significantly influence the model, the intercept alone should closely estimate the log of the mean of the dependent variable."
  },
  {
    "objectID": "projects/project3/index.html",
    "href": "projects/project3/index.html",
    "title": "The Multi-nomial Logit (MNL) Model",
    "section": "",
    "text": "This project uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project3/index.html#estimating-yogurt-preferences",
    "href": "projects/project3/index.html#estimating-yogurt-preferences",
    "title": "The Multi-nomial Logit (MNL) Model",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc. Let‚Äôs start with importing the necessary libraries for our project and read in the data.\nSample:\n\n\nCode\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize\n\n# Read in data\ndf = pd.read_csv('yogurt_data.csv')\nprint(df.head())\n\n\n   id  y1  y2  y3  y4  f1  f2  f3  f4     p1     p2     p3     p4\n0   1   0   0   0   1   0   0   0   0  0.108  0.081  0.061  0.079\n1   2   0   1   0   0   0   0   0   0  0.108  0.098  0.064  0.075\n2   3   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n3   4   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n4   5   0   1   0   0   0   0   0   0  0.125  0.098  0.049  0.079\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we‚Äôll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts‚Äô prices:\n\\[ x_j' = [\\mathbb{1}(\\text{Yogurt 1}), \\mathbb{1}(\\text{Yogurt 2}), \\mathbb{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a ‚Äúwide‚Äù shape with \\(n\\) rows and multiple columns for each covariate, to a ‚Äúlong‚Äù shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we‚Äôll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be ‚Äúpivoted‚Äù or ‚Äúmelted‚Äù from wide to long.\nTo achieve this we can call the Pandas function ‚Äòwide_to_long‚Äô. This function intuitively separates our original feature groups into new rows. One would expect that 100 rows and a feature group of 3 would result in a new dataframe of 100 x 3 = 300 rows. Likewise, our yogurt data was originally 2,430 rows and there are four possible yogurt brands. After pivoting, the new dataframe should be 9,720 rows.\n\n\nCode\n# Melt the dataset to convert from wide to long format\nlong_df = pd.wide_to_long(df, stubnames=['y', 'f', 'p'], \n                          i='id', j='product', \n                          sep='', suffix=r'\\d+').reset_index()\n\n# Generate dummy variables for the first three products\nfor i in range(1, 4):\n    long_df[f'brand_{i}'] = (long_df['product'] == i).astype(int)\n\n# Display the transformed dataframe\nprint(long_df.head())\nprint(long_df.shape)\n\n\n   id  product  y  f      p  brand_1  brand_2  brand_3\n0   1        1  0  0  0.108        1        0        0\n1   2        1  0  0  0.108        1        0        0\n2   3        1  0  0  0.108        1        0        0\n3   4        1  0  0  0.108        1        0        0\n4   5        1  0  0  0.125        1        0        0\n(9720, 8)\n\n\nTo spot check everything worked as intended, let‚Äôs print the rows where id = 1. This should show individual rows where y = 0 except for yogurt #4.\n\n\nCode\n# Show rows where id = 1\nprint(long_df[long_df['id'] == 1])\n\n\n      id  product  y  f      p  brand_1  brand_2  brand_3\n0      1        1  0  0  0.108        1        0        0\n2430   1        2  0  0  0.081        0        1        0\n4860   1        3  0  0  0.061        0        0        1\n7290   1        4  1  0  0.079        0        0        0\n\n\n\n\nEstimation\nIn order to fit our predict MNL model we need to define the log likelihood function. The log likelihood function is the natural logarithm of the likelihood function. The likelihood function itself measures the probability of observing the given data under specific values of the parameters in a statistical model.\n\n\nCode\ndef log_likelihood(beta, Y, X, ids):\n    # Calculate utilities from the features and beta coefficients\n    utilities = np.exp(X.dot(beta))\n\n    # Create a DataFrame to hold utilities and ids\n    df_util = pd.DataFrame({'utilities': utilities, 'id': ids})\n\n    # Calculate the sum of utilities for each ID\n    sum_utilities = df_util.groupby('id')['utilities'].transform('sum')\n\n    # Calculate probabilities\n    probabilities = utilities / sum_utilities\n\n    # Only take the log of probabilities where Y is 1 (chosen product)\n    chosen_probabilities = probabilities[Y == 1]\n\n    # Sum of log probabilities (log-likelihood)\n    log_likelihood_value = np.log(chosen_probabilities).sum()\n\n    return -log_likelihood_value  # Minimizing negative log-likelihood\n\n\nNow that that‚Äôs defined, the next step is find the parameter values that maximize the function. The output was intentionally negative so that calling it as an argument to the SciPy function minize() will maximize instead. Below we‚Äôll create variables to represent what we‚Äôre trying to find (y) and the independent variables (features).\n\n\nCode\n# Target variable\nY = long_df['y']\n\n# Independent variables\nX = long_df[['f', 'p', 'brand_1', 'brand_2', 'brand_3']]\n\n# We also need IDs to group calculations by consumers\nids = long_df['id']\n\n# Initial beta parameters\ninitial_beta = np.zeros(5)\n\n# Use scipy's minimize function to find MLEs\ninitial_beta = np.zeros(5) \nresult = minimize(log_likelihood, initial_beta, \nargs=(Y, X, ids), method='L-BFGS-B', \noptions={'gtol': 1e-6, 'maxiter': 500})\n\n\n\n\nDiscussion\nWe learn‚Ä¶\n\n\nCode\n# Check if the optimization was successful\nif result.success:\n    # Names corresponding to the coefficients\n    param_names = ['Featured', 'Price', 'Brand 1 Intercept', 'Brand 2 Intercept', 'Brand 3 Intercept']\n    \n    # Printing results\n    print(\"Optimization was successful.\")\n    print(\"Estimated parameters:\")\n    for name, value in zip(param_names, result.x):\n        print(f\"{name}: {value}\")\nelse:\n    print(\"Optimization failed.\")\n    print(f\"Reason: {result.message}\")\n\n\nOptimization was successful.\nEstimated parameters:\nFeatured: 0.48742878006589396\nPrice: -37.05740035791512\nBrand 1 Intercept: 1.3877239933483914\nBrand 2 Intercept: 0.6434883474619134\nBrand 3 Intercept: -3.086097997891248\n\n\nEach of the ‚ÄòBrand #‚Äô coefficients are relative to the base case of yogurt 4 (represented by data where brand_1, brand_2, and brand_3 are all equal to zero). Here‚Äôs a breakdown of what each means:\n\nBrand 1 Intercept (1.3877): This value is positive and the highest among the available brand intercepts, indicating that, all else being equal (such as price and whether the product was featured), Brand 1 is the most preferred yogurt compared to the base category (Yogurt 4). The positive value suggests that consumers have a strong preference for Brand 1 over Brand 4.\nBrand 2 Intercept (0.6435): Also positive, but lower than Brand 1‚Äôs intercept, indicating that Brand 2 is preferred over the base category (Yogurt 4) but less so than Brand 1. It‚Äôs still a favorable choice but not as strongly preferred as Brand 1.\nBrand 3 Intercept (-3.0861): This value is negative and quite large in magnitude, suggesting that Brand 3 is significantly less preferred than the base category (Yogurt 4). This indicates a strong aversion to Brand 3 compared to all the other brands.\n\nAll said, brand_1 is the most preferred yogurt, brand_2 is the second most preferred, and brand_3 is the least preferred. The fourth yogurt brand (brand_4) could be assumed to have a coefficient of zero and would lay between brand_2 and brand_3.\ntodo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\nNow all of these things mathematically representative of ‚Äòutils‚Äô. In economics and decision theory, ‚Äòutils‚Äô is a term used to represent a unit of utility. Utility, in this context, is an abstract measure that quantifies the satisfaction or happiness that a consumer derives from consuming goods and services. What can we do with this to drive decision making?\nWell, if use ‚Äòutils‚Äô as a conversion factor we can determine the effect of a feature‚Äôs coefficient in terms of another. Price is a great thing to compare to so we can talk to terms of dollars. A conversion factor for dollar-per-util can help calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept).\n\n\nCode\n# Convert price coefficient to utils\nprice_coef = 1 / result.x[1]\nprint(f\"Price coefficient in utils: {price_coef}\")\n\n# Calculate the difference in utility\nbrand_1_intercept = result.x[2]\nbrand_3_intercept = result.x[4]\n\nutility_diff = brand_1_intercept - brand_3_intercept\nprint(f\"Difference in utility between Brand 1 and Brand 3: {utility_diff}\")\n\n# Caculate the dollar benefit\nbenefit = utility_diff * price_coef \nprint(f\"Estimated dollar benefit between most and least-preferred yogurt: ${benefit:.2f}\")\n\n\nPrice coefficient in utils: -0.02698516329644287\nDifference in utility between Brand 1 and Brand 3: 4.473821991239639\nEstimated dollar benefit between most and least-preferred yogurt: $-0.12\n\n\nAfter converting the price coefficient to utils, the difference between the most and least popular yogurt brands can be said to be worth approximately $0.12 difference in price. This means that consumers are willing to pay $0.12 more for the most preferred yogurt brand compared to the least preferred brand, all else being equal.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nTo simulate this we‚Äôll calculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use the fitted model to predict p(y|x) for each consumer and each product.\n\n\nCode\n# Initial Market Shares\ninitial_market_shares = long_df.groupby('product')['y'].mean()  \n\n# Assuming results are stored in 'result.x'\nif result.success:\n    # Increase the price for yogurt 1\n    X_new = X.copy()\n    X_new.loc[X['brand_1'] == 1, 'p'] += 0.10\n\n    # Recalculate utilities with new coefficients\n    new_utilities = np.exp(X_new.dot(result.x))\n    df_new_util = pd.DataFrame({'utilities': new_utilities, 'id': ids})\n    sum_new_utilities = df_new_util.groupby('id')['utilities'].transform('sum')\n    new_probabilities = new_utilities / sum_new_utilities\n\n    # Map new probabilities back to each product\n    prob_df = pd.DataFrame({'product': long_df['product'], 'probability': new_probabilities})\n    new_market_shares = prob_df.groupby('product')['probability'].mean()\nelse:\n    print(\"Optimization failed:\", result.message)\n\n# Create a DataFrame to hold the results of initial and new market shares\nmarket_share_df = pd.DataFrame({'Initial Market Share': initial_market_shares, 'New Market Share': new_market_shares})\nmarket_share_df['Change in Market Share'] = market_share_df['New Market Share'] - market_share_df['Initial Market Share']\nprint(market_share_df)\n\n\n         Initial Market Share  New Market Share  Change in Market Share\nproduct                                                                \n1                    0.341975          0.021118               -0.320857\n2                    0.401235          0.591141                0.189907\n3                    0.029218          0.044041                0.014823\n4                    0.227572          0.343700                0.116128\n\n\nThe yogurt 1 market share decreases substantially in a scenario where the price is increased by $0.10 per oz. Shown above is the change in market share for each yogurt brand after the price increase in yogurt 1."
  },
  {
    "objectID": "projects/project3/index.html#estimating-minivan-preferences",
    "href": "projects/project3/index.html#estimating-minivan-preferences",
    "title": "The Multi-nomial Logit (MNL) Model",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\nNext we‚Äôll take a look at how a MNL model can be applied to data with multiple choice options from a survery of respondents. Step 1, load in the data (found at http://goo.gl/5xQObB) and take a look.\n\n\nCode\n# Load the data\ndf = pd.read_csv('rintro-chapter13conjoint.csv')\n\n# Display the first few rows of the dataframe\nprint(df.head())\n\n\n   resp.id  ques  alt carpool  seat cargo  eng  price  choice\n0        1     1    1     yes     6   2ft  gas     35       0\n1        1     1    2     yes     8   3ft  hyb     30       0\n2        1     1    3     yes     6   3ft  gas     30       1\n3        1     2    1     yes     6   2ft  gas     30       0\n4        1     2    2     yes     7   3ft  gas     35       1\n\n\n\n\nCode\nprint(df.shape)\nprint(df.dtypes)\n\n\n(9000, 9)\nresp.id     int64\nques        int64\nalt         int64\ncarpool    object\nseat        int64\ncargo      object\neng        object\nprice       int64\nchoice      int64\ndtype: object\n\n\n\n\nCode\nprint(df.nunique())\n\n\nresp.id    200\nques        15\nalt          3\ncarpool      2\nseat         3\ncargo        2\neng          3\nprice        3\nchoice       2\ndtype: int64\n\n\nFrom what we can see there are 200 respondents that participated in the survey. They were given fifteen choice tasks each. Each choice task presented three alternatives, each with a different combination of the four attributes (number of seats, cargo space, engine type, and price). The attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\nBefore continuing with the analysis one should check for null values in the dataset. If there are any, they should be removed or imputed.\n\n\nCode\ndf.isnull().sum()\n\n\nresp.id    0\nques       0\nalt        0\ncarpool    0\nseat       0\ncargo      0\neng        0\nprice      0\nchoice     0\ndtype: int64\n\n\n\n\nModel\nFor the minivan model we will omit the following levels to avoid multicollinearity: 6 seats, 2ft cargo, and gas engine. We will include price as a continuous variable. To do all of this we‚Äôll call on the mnlogit function from statsmodels. It‚Äôs not as fancy as writing our own function, but it‚Äôs a lot easier and gives us a nicely formatted summary output this time.\n\n\nCode\nfrom statsmodels.formula.api import mnlogit\n\n# Prepare the model formula, e.g., 'choice ~ carpool + C(seat) + C(cargo) + C(eng) + C(price)'\n# 'C()' indicates categorical treatment of the variable\nformula = 'choice ~ carpool + C(seat) + C(cargo) + C(eng) + price'\n\n# Fit a Multinomial Logit Model\nmodel = mnlogit(formula, data=df).fit()\n\n# Print the summary of the model to see the coefficient estimates\nprint(model.summary())\n\n\nOptimization terminated successfully.\n         Current function value: 0.558661\n         Iterations 6\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9000\nModel:                        MNLogit   Df Residuals:                     8992\nMethod:                           MLE   Df Model:                            7\nDate:                Thu, 16 May 2024   Pseudo R-squ.:                  0.1223\nTime:                        18:46:44   Log-Likelihood:                -5028.0\nconverged:                       True   LL-Null:                       -5728.6\nCovariance Type:            nonrobust   LLR p-value:                1.970e-298\n===================================================================================\n       choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept           4.0952      0.218     18.809      0.000       3.668       4.522\ncarpool[T.yes]      0.0086      0.053      0.162      0.872      -0.096       0.113\nC(seat)[T.7]       -0.5248      0.060     -8.800      0.000      -0.642      -0.408\nC(seat)[T.8]       -0.2931      0.059     -5.009      0.000      -0.408      -0.178\nC(cargo)[T.3ft]     0.4385      0.049      9.004      0.000       0.343       0.534\nC(eng)[T.gas]       1.4347      0.062     23.217      0.000       1.314       1.556\nC(eng)[T.hyb]       0.6742      0.063     10.715      0.000       0.551       0.798\nprice              -0.1591      0.006    -25.617      0.000      -0.171      -0.147\n===================================================================================\n\n\n\n\nResults\nThe coefficents for the various choices above represent the relative importance of each attribute in the decision-making process. Note that the number of options for each feature is n - 1 here. The base case is assumed to be zero and the other options move the needle relative to the base case. For example, the coefficients for seven or eight car seats is relative to six car seats (the option without a coefficient). The same goes for the other attributes.\nTo summarize in order: the carpool option appears to have very little effect on the respondents choice either way. Number of seats is negative for both the seven and eight seat options suggesting the base of six seats is the most desirable. The 3ft cargo space is preferred over 2ft. Of engine types gas is most popular, though hybrid is also preferred over electric. Price is negative, suggesting that as price increases the likelihood of choosing that option decreases. Coefficients of -0.8223 then -1.5866, at a glance, look like there is a negative linear relationship with the increase in price.\nLet‚Äôs take another look at converting the different options to price. To do this we will use the price coefficient as a dollar-per-util conversion factor. Then we will calculate the dollar value of 3ft of cargo space as compared to 2ft of cargo space using dollar-per-util to find the dollar value.\nSince there are only two features here, the coefficient for 3ft of cargo space represents the difference we‚Äôre looking for. I.E. 0.4386 - 0 = 0.4386. Finally, we‚Äôll multiply this coefficient by the price coefficient to get back to dollars and multiply again by 1000 since our pricing options are in thousands of dollars.\n\n\nCode\n# Convert price coefficient to utils\nprice_coef = 1 / model.params.iloc[-1].values[0]\nprint(f\"Price coefficient in utils: {price_coef}\")\n\n# Calculate the difference in utility\nutility_diff = 0.4386\nprint(f\"Difference in utility is: {utility_diff}\")\n\n# Caculate the dollar benefit\nbenefit = utility_diff * price_coef * 1000\nprint(f\"Estimated dollar benefit of additional cargo space: ${benefit:.2f}\")\n\n\nPrice coefficient in utils: -6.283823613187591\nDifference in utility is: 0.4386\nEstimated dollar benefit of additional cargo space: $-2756.09\n\n\nThis result means that the added utility of having an additional 1ft of cargo space (3ft vs.¬†2ft) is worth approximately $2756.09 to the consumer. The negative sign is an artifact of how the benefit is calculated and should be interpreted as the consumer being willing to pay up to $2756.09 more for the additional space. It essentially quantifies how much more valuable the car with more cargo space is, in monetary terms, compared to one with less space, all else being equal.\nThe model we created can also simulate hypothetical changes in our car data like we did previously. What would the market look like if the six minivans below were the available products? What could we predict the market share to be?\nModels:\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n\nCode\n# Data setup\ndata = {\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': [2, 2, 2, 3, 2, 2],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n}\n\ndf_predict = pd.DataFrame(data)\n\n# Convert `seat`, `cargo`, and `eng` to categorical types that match the training data\ndf_predict['cargo'] = df_predict['cargo'].map({2: '2ft', 3: '3ft'})\n\n# Add a 'carpool' column if missing, with default 'no' (adjust this based on your data setup)\nif 'carpool' not in df_predict.columns:\n    df_predict['carpool'] = 'no'\n\n# Check and ensure dummy encoding is correct by comparing with model training features\n# This step assumes the model has been trained and `model` is available\ntraining_features = model.params.index\npredict_features = pd.get_dummies(df_predict).columns\n\n\nWith the hypothetical data prepared to look like the training data we can use the predict function. This will create the probability matrix and take the averages like in the previous attempt. The output will be a dataframe of binary predictions for the selection of each minivan model.\n\n\nCode\n# Make predictions\nasdf = model.predict(df_predict)\nprint(asdf)\n\n\n          0         1\n0  0.629389  0.370611\n1  0.319582  0.680418\n2  0.386360  0.613640\n3  0.715438  0.284562\n4  0.906395  0.093605\n5  0.790060  0.209940\n\n\nTo see what these predictions mean for marketshare we will sum the probabilities and display them in relative terms.\n\n\nCode\n# DataFrame with probabilities from the model prediction\ndata = {\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'Prob_Not_Chosen': [0.629389, 0.319582, 0.386360, 0.715438, 0.906395, 0.790060],\n    'Prob_Chosen': [0.370611, 0.680418, 0.613640, 0.284562, 0.093605, 0.209940]\n}\ndf_predict = pd.DataFrame(data)\n\n# Sum of probabilities for being chosen\ntotal_probability_chosen = df_predict['Prob_Chosen'].sum()\n\n# Calculate market share for each minivan\ndf_predict['Market_Share'] = df_predict['Prob_Chosen'] / total_probability_chosen\n\n# Display the DataFrame with market shares\nprint(df_predict[['Minivan', 'Market_Share']])\n\n\n  Minivan  Market_Share\n0       A      0.164513\n1       B      0.302035\n2       C      0.272393\n3       D      0.126316\n4       E      0.041551\n5       F      0.093192\n\n\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\nMarket Share\n\n\n\n\nA\n7\n2\nHyb\n30\n16.45%\n\n\nB\n6\n2\nGas\n30\n30.20%\n\n\nC\n8\n2\nGas\n30\n27.24%\n\n\nD\n7\n3\nGas\n40\n12.63%\n\n\nE\n6\n2\nElec\n40\n4.16%\n\n\nF\n7\n2\nHyb\n35\n9.32%\n\n\n\nAccording to our model and given the training data, it is predicted that minivans A through F would take the above share of the market. This is a result of the predicted pros and cons of each feature combination relative to what we know of respondents choices given past information."
  },
  {
    "objectID": "projects/project4/index.html",
    "href": "projects/project4/index.html",
    "title": "Variable Importance",
    "section": "",
    "text": "Variable, or feature, importance in data anyltics refers to a set of metrics that indicate how important different predicitors are in contributing to the accuracy of a predictive model. Variable importance allows for (1) model simplification, (2) interpretability, and (3) eventual resouce allocation if the model is taken to an operational setting.\nToday we‚Äôll look at survey data regarding features of credit cards. Our goal is to measure variable importance in a number of different ways in order to better interpret how certain features of the credit cards influence overall customer satisfaction. To accomplish this we‚Äôll calculate and compare Pearson correlations, regression coefficients, ‚Äúusefullness‚Äù, Shapley values, Johnson‚Äôs relative weights, the mean decrease in the gini coefficient from a random forest, and XGBoost feature importance.\nThe first step, as always, is to import the various libraries needed and load the dataset.\n\n# Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport shap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('data_for_drivers_analysis.csv')\ndata.head()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nprint(data.shape)\nprint(data.dtypes)\nprint(data.nunique())\n\n(2553, 12)\nbrand           int64\nid              int64\nsatisfaction    int64\ntrust           int64\nbuild           int64\ndiffers         int64\neasy            int64\nappealing       int64\nrewarding       int64\npopular         int64\nservice         int64\nimpact          int64\ndtype: object\nbrand            10\nid              940\nsatisfaction      5\ntrust             2\nbuild             2\ndiffers           2\neasy              2\nappealing         2\nrewarding         2\npopular           2\nservice           2\nimpact            2\ndtype: int64\n\n\nFrom the preview and summary statistics above we can see there are 2553 rows of data. Within the data ten brands were represented. For each brand a customer satisfaction score of between 1 and 5 was assigned. The remaining columns show whether or not the respondent answered affirmatively to these questions:\n\nIs offered by a brand I trust\nHelps build credit quickly\nIs different from other cards\nIs easy to use\nHas appealing benefits or rewards\nRewards me for responsible usage\nIs used by a lot of people\nProvides outstanding customer service\nMakes a difference in my life\n\nIn order to analyze the data we‚Äôll separate the X and y variables. X will be the various features with binary outcomes wheras y is the dependent variable, satisfaction.\n\n# Define the binary columns and the target column\nbinary_columns = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\ntarget_column = 'satisfaction'\n\nX = data[binary_columns]\ny = data[target_column]\n\n# Adding constant for regression model in statsmodels\nX_const = sm.add_constant(X)\n\n\nPearson Correlation\nPearson correlation coefficient between two variables (X) and (Y) is defined as: \\[\nr = \\frac{\\sum (X - \\bar{X})(Y - \\bar{Y})}{\\sqrt{\\sum (X - \\bar{X})^2 \\sum (Y - \\bar{Y})^2}}\n\\]\nPearson correlation measures the linear correlation between each independent variable and the target variable. A Pearson correlation coefficient ranges from -1 to +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship. This method is straightforward and widely used for preliminary analysis to gauge linear associations but does not capture non-linear relationships or interactions between features.\n\n# Pearson Correlations\npearson_corr = X.apply(lambda x: x.corr(data[target_column]))\n\n\n\nRegression Coefficients\nThe linear regression model can be succinctly expressed in mathematical terms as follows:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\epsilon\n\\]\nDerived from fitting a linear regression model, these coefficients quantify the expected change in the target variable for a one-unit change in the predictor variable, assuming other variables are held constant. Regression coefficients are useful for understanding the direct impact of each predictor in a linear context. However, like Pearson correlations, it‚Äôs limited to linear effects and may be misleading in the presence of multicollinearity.\n\n# Linear Regression for regression coefficients\nmodel = sm.OLS(y, X_const).fit()\nregression_coefficients = model.params.drop('const')\n\n\n\n‚ÄúUsefullness‚Äù\nThe usefulness of a feature ( X_j ) is quantified by the change in ( R^2 ) when the feature is removed: \\[\n\\text{Usefulness}_j = R^2_{\\text{full model}} - R^2_{\\text{model without } X_j}\n\\]\nThe concept of ‚Äúusefulness‚Äù in the context of regression analysis is a method of quantifying the importance of each predictor variable by evaluating the impact of removing each variable from the model. It measures how much the overall model‚Äôs explanatory power, typically quantified by the ùëÖ^2 value, decreases when a specific feature is omitted.\n\n# Calculate changes in R¬≤ (usefulness)\nfull_r_squared = model.rsquared\nusefulness = {}\nfor col in binary_columns:\n    reduced_model = sm.OLS(y, X_const.drop(columns=[col])).fit()\n    usefulness[col] = full_r_squared - reduced_model.rsquared\nusefulness_series = pd.Series(usefulness)\n\n\n\nShapley values\nThe Shapley value for a feature (X_i) in a prediction model is given by: \\[\n\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N|-|S|-1)!}{|N|!} (v(S \\cup \\{i\\}) - v(S))\n\\] where (S) is a subset of all features except (i), (N) is the set of all features, and (v(S)) is the prediction model output with features in set (S).\nShapley values measure the contribution of each feature to the prediction of a model at the individual instance level. This method provides a detailed and nuanced explanation of how each feature in a dataset influences the model‚Äôs predictions, allowing for a deep understanding of model behavior, which is especially valuable in complex models where interactions and non-linearities are present.\n\n# Shapley Values using Linear Regression model\nmodel = LinearRegression().fit(X, y)\nexplainer = shap.Explainer(model, X)\nshap_values = explainer(X)\nmean_shap_values = pd.DataFrame(shap_values.values, columns=binary_columns).abs().mean().values\n\n\n\nJohnson‚Äôs Relative Weights\nJohnson‚Äôs relative weights involve transforming the eigenvalues ( ) and eigenvectors ( V ) from the correlation matrix of the predictors ( R ): \\[\n\\text{Relative Weights} = \\frac{V^2 \\times \\lambda}{\\sum (V^2 \\times \\lambda)}\n\\]\nThis method involves an eigen decomposition of the correlation matrix of the predictors and calculates the proportionate contribution of each predictor to the R¬≤ of a regression model. It effectively parses out each variable‚Äôs importance, taking into account multicollinearity.\n\n# Johnson's Relative Weights \ncorr_matrix = np.corrcoef(X, rowvar=False)\neigenvalues, eigenvectors = np.linalg.eig(corr_matrix)\nsmc = 1 - 1 / np.diag(np.linalg.inv(corr_matrix))\nrel_weights = (eigenvectors**2 @ eigenvalues) * smc\nrelative_weights = rel_weights / rel_weights.sum()\n\n\n\nRandom Forest\nIn Random Forest, feature importance is typically assessed by how much each feature decreases the impurity of the nodes (e.g., using Gini impurity), averaged across all trees in the forest. \\[\n\\text{Importance} = \\frac{\\text{Total Decrease in Impurity caused by feature } X}{\\text{all splits}}\n\\]\nThis method is excellent for capturing non-linear relationships and interactions without any assumptions about data distribution, and it is robust to outliers and scale of features.\n\n# Random Forest for feature importance\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrf = RandomForestClassifier(n_estimators=1000, random_state=42)\nrf.fit(X_train, y_train)\nrf_importances = rf.feature_importances_\n\n\n\nXGBoost\nSimilar to Random Forest, but often focused on gain, which measures the contribution of each feature to the model by the increase in purity: \\[\n\\text{Gain} = \\frac{\\text{Total Gain of splits using feature } X}{\\text{all splits using feature } X}\n\\]\nThe importance is computed for each feature at each split in each tree and averaged across all trees.\n\n# XGBoost Model\ny_train_adjusted = y_train - 1\ny_test_adjusted = y_test - 1\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\nxgb_model.fit(X_train, y_train_adjusted)\nxgb_importances = xgb_model.feature_importances_\n\nIn order to finalize the comparison, each variable importance will be plotted side by side. For any methods which do not sum to 100%, we‚Äôll normalize the output to better visualize the relative percentage of importance to customer satisfaction.\n\n# Creating DataFrame\nsummary_df = pd.DataFrame({\n    'Pearson Correlations': pearson_corr,\n    'Regression Coefficients': regression_coefficients,\n    'Usefulness': usefulness_series,\n    'Shapley Values': mean_shap_values,\n    \"Johnson's Relative Weights\": relative_weights,  \n    'Random Forest': rf_importances,\n    'XGBoost': xgb_importances\n})\n\n# Normalize the values by the sum to scale them as per your previous output (except for RF and Johnson's)\nsummary_df['Pearson Correlations'] /= summary_df['Pearson Correlations'].sum()\nsummary_df['Regression Coefficients'] /= summary_df['Regression Coefficients'].sum()\nsummary_df['Usefulness'] /= summary_df['Usefulness'].sum()\nsummary_df['Shapley Values'] /= summary_df['Shapley Values'].sum()\nsummary_df['XGBoost'] /= summary_df['XGBoost'].sum()\n\n\n# Plotting summary_df as a heatmap\ndescriptions = {\n    'trust': 'Is offered by a brand I trust',\n    'build': 'Helps build credit quickly',\n    'differs': 'Is different from other cards',\n    'easy': 'Is easy to use',\n    'appealing': 'Has appealing benefits or rewards',\n    'rewarding': 'Rewards me for responsible usage',\n    'popular': 'Is used by a lot of people',\n    'service': 'Provides outstanding customer service',\n    'impact': 'Makes a difference in my life'\n}\n\nsummary_df = summary_df.rename(index=descriptions)\n\nplt.figure(figsize=(6, 4)) \nax = sns.heatmap(summary_df, annot=True, cmap='Greens', fmt=\".2f\", linewidths=.5, cbar=False)\nax.xaxis.set_ticks_position('top')  \nax.xaxis.set_label_position('top')  \nplt.xticks(rotation=45, ha='left')  \nplt.xlabel('Feature Importance Metrics')  \nplt.ylabel('Features')  \nplt.show()"
  }
]