{"title":"A Replication of Karlan and List (2007)","markdown":{"yaml":{"title":"A Replication of Karlan and List (2007)","author":"Robin Reese","date":"today","callout-appearance":"minimal","format":{"html":{"code-fold":true,"code-summary":"Show the code"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).\n\nIn this field experiment involving over 50,000 prior donors to a non-profit, the researchers investigate how offering matching grants affects charitable giving. Two-thirds of the sample were placed into a treatment group, receiving solicitation letters highlighting that their donations would be matched by an anonymous donor at various ratios and maximum amounts, while the remaining third, serving as a control group, received standard solicitation materials without mention of a match. The treatment group was further stratified into subgroups, each receiving tailored messaging about the matching ratio, the cap on the matching grant, and suggested donation amounts based on their previous giving history.\n\nThe study's key outcome measures were the response rate to the solicitation and the amount of money donated. By integrating subtle variations in the solicitation letter and the accompanying reply card, the experiment meticulously isolates the influence of the matching grant information. The results offer actionable insights into the effectiveness of different fundraising strategies, revealing the intricate ways in which donors' decisions are influenced not only by the economic benefit of matching grants but also by their previous engagement levels and the manner in which the opportunity to have their donation matched is communicated.\n\nThis project seeks to replicate their results.\n\n\n## Data\n\n### Description\n\nThe first step in our analysis of the data will be to load and explore the results. Below we bring in the state file and take a quick overview of the variables' counts, types, and distributions.\n\n```{python}\n\n# Read the .dta file\nimport pandas as pd\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# Get an overview of the data\ndf.info()\n\n# Describe the data\ndata_description = df.describe()\nprint(data_description)\n\n\n```\n\n\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n\n### Balance Test \n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nDirect t-tests and t-tests from linear regression are both statistical tools used to assess the significance of differences between groups or the impact of predictors. A direct t-test compares the means of two independent samples to determine if they come from distributions with equal means, making it ideal for straightforward comparisons between two groups, such as control and treatment conditions in an experiment. The resulting p-value indicates whether any observed difference is likely to be due to chance.\n\nIn contrast, a t-test from a linear regression analysis evaluates the significance of individual predictors within a more complex model that may include multiple variables. The t-statistic here assesses whether a coefficient differs significantly from zero, taking into account other factors in the model. This allows for the evaluation of each predictor's unique contribution and the control of confounding variables. Both types of t-tests rely on the assumption of normally distributed errors and can be adapted for equal or unequal variances between groups. While direct t-tests are best suited for simpler experimental designs, regression t-tests excel in multifaceted studies where multiple influences need to be accounted for simultaneously.\n\nWe'll define functions next that are able to perform both test methodologies on our dataset as needed.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef direct_t_test(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data for consistency with regression\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Separate the treatment and control groups\n    treatment_group = df[df[treatment_col] == 1][outcome_col]\n    control_group = df[df[treatment_col] == 0][outcome_col]\n\n    # Calculate means\n    mean_treatment = treatment_group.mean()\n    mean_control = control_group.mean()\n\n    # Calculate standard deviations\n    std_treatment = treatment_group.std(ddof=1)\n    std_control = control_group.std(ddof=1)\n\n    # Calculate sample sizes\n    n_treatment = len(treatment_group)\n    n_control = len(control_group)\n\n    # Calculate separate standard errors\n    se_treatment = std_treatment / np.sqrt(n_treatment)\n    se_control = std_control / np.sqrt(n_control)\n\n    # Calculate the t-statistic\n    t_stat = (mean_treatment - mean_control) / np.sqrt(se_treatment**2 + se_control**2)\n\n    # Calculate degrees of freedom using the Welch-Satterthwaite equation\n    df = ((se_treatment**2 + se_control**2)**2 /\n          ((se_treatment**4 / (n_treatment - 1)) + (se_control**4 / (n_control - 1))))\n\n    return t_stat, df\n\ndef run_regression(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Prepare the design matrix X with a constant (intercept) and the treatment indicator\n    X = sm.add_constant(df[treatment_col])\n    Y = df[outcome_col]\n\n    # Fit the OLS regression model\n    model = sm.OLS(Y, X).fit()\n\n    # Extract the t-statistic and p-value for the treatment variable\n    t_stat = model.tvalues[treatment_col]\n\n    # Return the t-statistic and degrees of freedom\n    return t_stat, model.df_resid\n\n# Example usage (you need to replace 'your_dataframe', 'treatment', and 'outcome' with your actual DataFrame and column names)\n# t_stat_direct, df_direct = direct_t_test(your_dataframe, 'treatment', 'outcome')\n# t_stat_regression, df_regression = run_regression(your_dataframe, 'treatment', 'outcome')\n# print(\"Direct t-test:\", t_stat_direct, \"df:\", df_direct)\n# print(\"Regression t-test:\", t_stat_regression, \"df:\", df_regression)\n\n\n```\n\n\n_todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\n\nNow that the functions are defined, we can test different variables besides the outcome variables to see if the treatment and control groups show significant differences from each other.\n\n\n```{python}\n\n# Prepare groups for direct t-test of 'mrm2'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'mrm2')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'mrm2')\nprint('Testing the mrm2 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'female'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'female')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'female')\nprint('Testing the female variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'red0'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'red0')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'red0')\nprint('Testing the red0 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'years')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'years')\nprint('Testing the years variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n```\n\n\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nAnalyzing whether matched donations lead to an increased response rate of making a donation is a critical first step in reproducing the results of a study because it directly assesses the effectiveness of the matching incentive as a motivational tool in charitable giving. This analysis establishes the foundational evidence needed to validate the underlying assumptions of fundraising strategies that utilize matching offers to enhance donor engagement and generosity. Below we'll plot the differences in response rate between the treatment and control groups.\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# Calculate the proportions for the 'gave' column based on the 'treatment' indicator\nproportions = df.groupby('treatment')['gave'].mean()\n\n# Create a barplot\nplt.figure(figsize=(10, 6))\ncolors = ['#1f77b4', '#ff7f0e']  # Aesthetically pleasing color palette\nproportions.plot(kind='bar', color=colors)\n\n# Add labels and title\nplt.title('Proportion of People Who Donated by Group')\nplt.xlabel('Group')\nplt.ylabel('Proportion of Donations')\nplt.xticks(ticks=[0, 1], labels=['Control', 'Treatment'], rotation=0)  # Rename x-ticks for clarity\n\n# Show the plot\nplt.show()\n```\n\nIt appears from a visualization that there is a difference. The treatment group has a slightly higher response rate than does the control group. How sure are we that this effect size is statistically significant rather than due to random chance?\n\nTo determine this the next step is to run a t-test between the treatment and control groups on the donation outcome. Using the binary variable 'gave'. This designates whether a person gave a donation in any amount.\n\n\n```{python}\nfrom scipy import stats\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'gave')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'gave')\nprint('Testing the gave variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n\n```\n\nThe above results show stastical values that lead one to believe the difference in response rates between the treatment and control groups are unlikely to have occurred by chance. It indicates that such financial incentives can effectively influence human behavior, enhancing the likelihood of donating or possibly increasing the donation amounts.\n\n_todo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._\n\n```{python}\nimport statsmodels.formula.api as smf\n\n# Run the probit regression model\ndef run_probit_regression(df, formula):\n    # Probit model using the formula interface\n    model = smf.probit(formula, data=df)\n    results = model.fit(disp=0)\n    return results\n\n# Use the probit regression function\nformula = 'gave ~ treatment'\nprobit_results = run_probit_regression(df, formula)\n\n# Print out the summary of the regression results\ncoefficients = probit_results.params\np_values = probit_results.pvalues\nprint(\"Treatment Coefficient: \", coefficients[1])\nprint(\"Treatment P-Value: \", p_values[1])\n```\n\n### Differences between Match Rates\n\nIn the realm of charitable giving, the size of matched donations often plays a pivotal role in incentivizing potential donors. Understanding how different matching ratios influence donor behavior can provide valuable insights for optimizing fundraising strategies. This analysis focuses on evaluating the impact of various match rates—1:1, 2:1, and 3:1—on the likelihood of donations. By employing a series of t-tests, the study seeks to determine if higher match ratios significantly increase the response rate among donors, thereby testing the assumption that more generous matching offers might lead to higher participation rates in donation campaigns.\n\nBelow is the code to assess the differences in rates of response between different matching offers presented:\n\n```{python}\nfrom scipy import stats\n\n# Calculate the mean donation rate for each ratio category\nmean_ratio1 = df[df['ratio'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Conduct t-tests\nt_test_1_vs_2 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio2'] == 1]['gave'], equal_var=False)\nt_test_1_vs_3 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\nt_test_2_vs_3 = stats.ttest_ind(df[df['ratio2'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\n\n# Print the results\nprint(f\"Mean Donation Rate for 1:1 match: {mean_ratio1:.4f}\")\nprint(f\"Mean Donation Rate for 2:1 match: {mean_ratio2:.4f}\")\nprint(f\"Mean Donation Rate for 3:1 match: {mean_ratio3:.4f}\")\nprint(\"T-test Results:\")\nprint(\"1:1 vs 2:1:\", t_test_1_vs_2)\nprint(\"1:1 vs 3:1:\", t_test_1_vs_3)\nprint(\"2:1 vs 3:1:\", t_test_2_vs_3)\n\n```\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors' comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\n\nNext we'll use regression to look at the comparison of mean response rates for different matching ratios on whether people chose to donate. The coefficient of each variable [ratio1, ratio2, ratio3] can be looked at to determine the effect size between levels of matching presented. The analysis gives us a p-value as well to determine the significance of the movement suggested by the coefficient.\n\n```{python}\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n# Prepare the design matrix X with a constant and the dummy variables for ratio\nX = sm.add_constant(df[['ratio1', 'ratio2', 'ratio3']])\nY = df['gave']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n```\n\n\nInteresting. The summary output will be returned to shortly. Next we'll compare the mean response rates between the matching offers. This will show how much the response rate moves as the offer is increased from 1:1 to 2:1 to 3:1.\n\n```{python}\nmean_ratio1 = df[df['ratio1'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Calculate the differences directly from the data\ndifference_1_to_2 = mean_ratio2 - mean_ratio1\ndifference_2_to_3 = mean_ratio3 - mean_ratio2\n\nprint(f\"Directly calculated difference in response rate from 1:1 to 2:1 match ratio: {difference_1_to_2:.4f}\")\nprint(f\"Directly calculated difference in response rate from 2:1 to 3:1 match ratio: {difference_2_to_3:.4f}\")\n```\n\nFor housekeeping we should look to see if the model coefficients show a similar movement to the mean comparisons.\n\n```{python}\ncoefficients = model.params\n\n# Calculate the difference between ratio2 - ratio1 and ratio3 - ratio2\ndifference_ratio2_ratio1 = coefficients['ratio2'] - coefficients['ratio1']\ndifference_ratio3_ratio2 = coefficients['ratio3'] - coefficients['ratio2']\n\ndifference_ratio2_ratio1, difference_ratio3_ratio2\n\nprint(f\"Coefficient comparison difference in response rate from 1:1 to 2:1 match ratio: {difference_ratio2_ratio1:.4f}\")\nprint(f\"Coefficient comparison difference in response rate from 2:1 to 3:1 match ratio: {difference_ratio3_ratio2:.4f}\")\n\n```\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors' comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution. In order to do this we shall run the same t-test only this time we're using the donation amount relative to the treatment effect.\n\n```{python}\n\n# Perform both t-tests of 'amount'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'amount')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'amount')\nprint('Testing the amount variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n```\n\nThe analysis suggests that matched donations have a potentially positive, though not statistically significant, impact on donation amounts at the traditional 5% significance level. Given the proximity of the p-value to this threshold, organizations might still consider matched donations as part of a broader, diversified fundraising strategy. Barring additional research, it may yet prove a positive influence on donation amounts.\n\nAs a next step we'll filter the data to only show those who donated something. This can then be compared to the treatment variable to see if treatment has an effect on the amount given that can be teased out of the data. \n\n```{python}\n# Filter to include only rows where a positive donation was made\ndf_donors = df[df['amount'] > 0]\n\n# Prepare the design matrix X with a constant and the treatment indicator\nX = sm.add_constant(df_donors['treatment'])\nY = df_donors['amount']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n```\n\nThe coefficient for the treatment variable, which is negative, suggests that being in the treatment group (i.e., offered a matched donation) is associated with a decrease in the donation amount by approximately $1.67 compared to the control group. However, the p-value associated with this coefficient is 0.561, indicating that this effect is not statistically significant.\n\nThe regression analysis reveals that, contrary to expectations, the treatment (matched donations) does not lead to an increase in the amount donated. Instead, there's an indication (though not statistically significant) that it might decrease the amount donated if we are to read the outputs of the regression. All in all, there is not enough information here to say that donation amount specifically is moved by the treatment.\n\n```{python}\n\ntreatment_donated = df_donors[df_donors['treatment'] == 1]\ncontrol_donated = df_donors[df_donors['treatment'] == 0]\n\n# Calculate the mean donation amount for each group\nmean_treatment = treatment_donated['amount'].mean()\nmean_control = control_donated['amount'].mean()\n\n# Creating histograms\nplt.figure(figsize=(14, 6))\n\n# Histogram for treatment group\nplt.subplot(1, 2, 1)\nplt.hist(treatment_donated['amount'], bins=30, color='blue', alpha=0.7)\nplt.axvline(mean_treatment, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Treatment Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_treatment:.2f}', xy=(mean_treatment, 50),\n             xytext=(mean_treatment + 50, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='right')\n\n# Histogram for control group\nplt.subplot(1, 2, 2)\nplt.hist(control_donated['amount'], bins=30, color='green', alpha=0.7)\nplt.axvline(mean_control, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Control Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_control:.2f}', xy=(mean_control, 20),\n             xytext=(mean_control - 150, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='left')\n\nplt.tight_layout()\nplt.show()\n```\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\nIn the provided text, the author outlines an educational simulation designed to illustrate key statistical principles, namely the Law of Large Numbers and the Central Limit Theorem, using a practical example from a charitable donation context. By setting up a scenario where the probabilities of making a donation differ between respondents who receive a match and those who do not, the simulation aims to show how differences in probabilities influence donation behaviors over a large number of trials. Specifically, respondents who do not receive a match have a lower probability (0.018) of donating compared to those who receive a match (0.022).\n\nThe code below is structured to generate a substantial number of simulations—10,000 for each group—to model donation outcomes according to the specified Bernoulli distributions. It uses Python’s numpy library to simulate these outcomes, ensuring reproducibility by setting a random seed. Once the donation data for both control (no match) and treatment (match) groups are simulated, the script calculates the cumulative average of the differences in donation probabilities between the two groups across the number of draws. This method allows the plot to visually depict how the average differences evolve as more data points are considered, highlighting the convergence behavior predicted by the Law of Large Numbers.\n\n```{python}\n# Set the true probabilities for control and treatment\ntrue_prob_control = 0.018\ntrue_prob_treatment = 0.022\n\n# Number of simulations/draws\nnum_simulations = 10000\n\n# Simulate donations for control and treatment groups\nnp.random.seed(0)  # For reproducibility\ncontrol_donations = np.random.binomial(1, true_prob_control, num_simulations)\ntreatment_donations = np.random.binomial(1, true_prob_treatment, num_simulations)\n\n# Compute the cumulative average of the differences\ncumulative_differences = np.cumsum(treatment_donations - control_donations) / np.arange(1, num_simulations + 1)\n\n# Plot the cumulative averages\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_differences, color='red', lw=2)\nplt.axhline(y=true_prob_treatment - true_prob_control, color='blue', lw=1, linestyle='--')\n\nplt.xlabel('Number of Draws')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Cumulative Average Difference in Donation Rate')\nplt.show()\n\nprint(\"The mean difference is \" + str(true_prob_treatment - true_prob_control) + \" (the blue hash line).\")\n```\n\nThe plot generated by the code visually confirms the theoretical expectation: as the number of simulations increases, the cumulative average of the differences should approach the true difference in means (0.004) between the control and treatment groups. This is depicted on the plot by the blue dashed line at the level of the true mean difference. The exercise not only reinforces the statistical theory behind sampling distributions and their averages but also provides an intuitive grasp of how small probability differences can be detected and quantified with sufficient data, underlining the practical applications of these concepts in analyzing and interpreting data from real-world experiments. This simulation thereby serves as a powerful tool in both teaching and understanding statistical inference through a direct and engaging approach.\n\n### Central Limit Theorem\n\n\nThe Central Limit Theorem (CLT) is a fundamental principle in statistics that plays a pivotal role in the simulation above. It states that, regardless of the distribution of the population, the distribution of the sample means will approximate a normal distribution as the sample size increases, provided the samples are independent and identically distributed with a finite mean and variance. This theorem is crucial because it justifies the use of normal probability theory in the inference about the mean of a population, even when the population itself is not normally distributed.\n\nIn this scenario, the Central Limit Theorem comes into play by ensuring that the distribution of the cumulative average differences between the treatment and control groups will approach a normal distribution as the number of draws (i.e., sample size) increases. Each draw represents a Bernoulli trial where a donation is made with a certain probability. As you simulate more trials, the average of these results (due to the law of large numbers) will converge not only towards the true mean difference but also the distribution of these averages will start to resemble a normal distribution (thanks to the CLT).\n\n```{python}\n# Let's generate the histograms for the sample sizes specified and comment on their distribution.\n\n# Function to simulate the process and calculate the averages\ndef simulate_averages(sample_size, repetitions, p_control, p_treatment):\n    control_means = np.random.binomial(sample_size, p_control, repetitions) / sample_size\n    treatment_means = np.random.binomial(sample_size, p_treatment, repetitions) / sample_size\n    return treatment_means - control_means\n\n# Sample sizes to generate histograms for\nsample_sizes = [50, 200, 500, 1000]\n\n# Control and treatment probabilities\np_control = 0.018\np_treatment = 0.022\n\n# Number of repetitions to calculate averages\nrepetitions = 1000\n\n# Set up the plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# Generate the histograms\nfor i, sample_size in enumerate(sample_sizes):\n    # Simulate the differences in averages\n    avg_differences = simulate_averages(sample_size, repetitions, p_control, p_treatment)\n    \n    # Plot histogram horizontally\n    axes[i].hist(avg_differences, bins=30, orientation='horizontal', color='skyblue', edgecolor='black')\n    mean_value = np.mean(avg_differences)\n    axes[i].axhline(y=mean_value, color='red', linestyle='dashed', linewidth=2)\n    \n    # Annotate the mean difference\n    axes[i].annotate(f'Mean: {mean_value:.4f}', xy=(0.5, mean_value), xytext=(10, 0), \n                     textcoords='offset points', fontsize=10, color='red', ha='center', va='bottom')\n    \n    # Set title and labels\n    axes[i].set_title(f'Sample size: {sample_size}')\n    axes[i].set_ylabel('Average Difference')\n    axes[i].set_xlabel('Frequency')\n\n# Adjust the layout\nplt.tight_layout()\nplt.show()\n```\n\nFor each histogram corresponding to different sample sizes (50, 200, 500, 1000), zero should ideally be within the tails of the distribution if there is no true difference. \n\nSince the true probabilities differ by 0.004 (p_treatment - p_control), zero is not expected to be in the center if the simulation reflects the true difference.\n\nAs sample size increases, the distribution of the average differences should become more centered around the true difference (0.004), and the variance should decrease, making the distribution narrower around the mean. Though there is an element of randomness in each sample, we can see that as n increases the peak of the histogram starts to converge around the mean value we know to be 0.004.\n\n\n\n","srcMarkdownNoYaml":"\n\n\n## Introduction\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).\n\nIn this field experiment involving over 50,000 prior donors to a non-profit, the researchers investigate how offering matching grants affects charitable giving. Two-thirds of the sample were placed into a treatment group, receiving solicitation letters highlighting that their donations would be matched by an anonymous donor at various ratios and maximum amounts, while the remaining third, serving as a control group, received standard solicitation materials without mention of a match. The treatment group was further stratified into subgroups, each receiving tailored messaging about the matching ratio, the cap on the matching grant, and suggested donation amounts based on their previous giving history.\n\nThe study's key outcome measures were the response rate to the solicitation and the amount of money donated. By integrating subtle variations in the solicitation letter and the accompanying reply card, the experiment meticulously isolates the influence of the matching grant information. The results offer actionable insights into the effectiveness of different fundraising strategies, revealing the intricate ways in which donors' decisions are influenced not only by the economic benefit of matching grants but also by their previous engagement levels and the manner in which the opportunity to have their donation matched is communicated.\n\nThis project seeks to replicate their results.\n\n\n## Data\n\n### Description\n\nThe first step in our analysis of the data will be to load and explore the results. Below we bring in the state file and take a quick overview of the variables' counts, types, and distributions.\n\n```{python}\n\n# Read the .dta file\nimport pandas as pd\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# Get an overview of the data\ndf.info()\n\n# Describe the data\ndata_description = df.describe()\nprint(data_description)\n\n\n```\n\n\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n\n### Balance Test \n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nDirect t-tests and t-tests from linear regression are both statistical tools used to assess the significance of differences between groups or the impact of predictors. A direct t-test compares the means of two independent samples to determine if they come from distributions with equal means, making it ideal for straightforward comparisons between two groups, such as control and treatment conditions in an experiment. The resulting p-value indicates whether any observed difference is likely to be due to chance.\n\nIn contrast, a t-test from a linear regression analysis evaluates the significance of individual predictors within a more complex model that may include multiple variables. The t-statistic here assesses whether a coefficient differs significantly from zero, taking into account other factors in the model. This allows for the evaluation of each predictor's unique contribution and the control of confounding variables. Both types of t-tests rely on the assumption of normally distributed errors and can be adapted for equal or unequal variances between groups. While direct t-tests are best suited for simpler experimental designs, regression t-tests excel in multifaceted studies where multiple influences need to be accounted for simultaneously.\n\nWe'll define functions next that are able to perform both test methodologies on our dataset as needed.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef direct_t_test(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data for consistency with regression\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Separate the treatment and control groups\n    treatment_group = df[df[treatment_col] == 1][outcome_col]\n    control_group = df[df[treatment_col] == 0][outcome_col]\n\n    # Calculate means\n    mean_treatment = treatment_group.mean()\n    mean_control = control_group.mean()\n\n    # Calculate standard deviations\n    std_treatment = treatment_group.std(ddof=1)\n    std_control = control_group.std(ddof=1)\n\n    # Calculate sample sizes\n    n_treatment = len(treatment_group)\n    n_control = len(control_group)\n\n    # Calculate separate standard errors\n    se_treatment = std_treatment / np.sqrt(n_treatment)\n    se_control = std_control / np.sqrt(n_control)\n\n    # Calculate the t-statistic\n    t_stat = (mean_treatment - mean_control) / np.sqrt(se_treatment**2 + se_control**2)\n\n    # Calculate degrees of freedom using the Welch-Satterthwaite equation\n    df = ((se_treatment**2 + se_control**2)**2 /\n          ((se_treatment**4 / (n_treatment - 1)) + (se_control**4 / (n_control - 1))))\n\n    return t_stat, df\n\ndef run_regression(df, treatment_col, outcome_col):\n    # Drop any rows with missing values and ensure numeric data\n    df = df.dropna(subset=[treatment_col, outcome_col])\n    df[treatment_col] = pd.to_numeric(df[treatment_col], errors='coerce')\n    df[outcome_col] = pd.to_numeric(df[outcome_col], errors='coerce')\n\n    # Prepare the design matrix X with a constant (intercept) and the treatment indicator\n    X = sm.add_constant(df[treatment_col])\n    Y = df[outcome_col]\n\n    # Fit the OLS regression model\n    model = sm.OLS(Y, X).fit()\n\n    # Extract the t-statistic and p-value for the treatment variable\n    t_stat = model.tvalues[treatment_col]\n\n    # Return the t-statistic and degrees of freedom\n    return t_stat, model.df_resid\n\n# Example usage (you need to replace 'your_dataframe', 'treatment', and 'outcome' with your actual DataFrame and column names)\n# t_stat_direct, df_direct = direct_t_test(your_dataframe, 'treatment', 'outcome')\n# t_stat_regression, df_regression = run_regression(your_dataframe, 'treatment', 'outcome')\n# print(\"Direct t-test:\", t_stat_direct, \"df:\", df_direct)\n# print(\"Regression t-test:\", t_stat_regression, \"df:\", df_regression)\n\n\n```\n\n\n_todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\n\nNow that the functions are defined, we can test different variables besides the outcome variables to see if the treatment and control groups show significant differences from each other.\n\n\n```{python}\n\n# Prepare groups for direct t-test of 'mrm2'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'mrm2')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'mrm2')\nprint('Testing the mrm2 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'female'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'female')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'female')\nprint('Testing the female variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Prepare groups for direct t-test of 'red0'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'red0')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'red0')\nprint('Testing the red0 variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'years')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'years')\nprint('Testing the years variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n```\n\n\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nAnalyzing whether matched donations lead to an increased response rate of making a donation is a critical first step in reproducing the results of a study because it directly assesses the effectiveness of the matching incentive as a motivational tool in charitable giving. This analysis establishes the foundational evidence needed to validate the underlying assumptions of fundraising strategies that utilize matching offers to enhance donor engagement and generosity. Below we'll plot the differences in response rate between the treatment and control groups.\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# Calculate the proportions for the 'gave' column based on the 'treatment' indicator\nproportions = df.groupby('treatment')['gave'].mean()\n\n# Create a barplot\nplt.figure(figsize=(10, 6))\ncolors = ['#1f77b4', '#ff7f0e']  # Aesthetically pleasing color palette\nproportions.plot(kind='bar', color=colors)\n\n# Add labels and title\nplt.title('Proportion of People Who Donated by Group')\nplt.xlabel('Group')\nplt.ylabel('Proportion of Donations')\nplt.xticks(ticks=[0, 1], labels=['Control', 'Treatment'], rotation=0)  # Rename x-ticks for clarity\n\n# Show the plot\nplt.show()\n```\n\nIt appears from a visualization that there is a difference. The treatment group has a slightly higher response rate than does the control group. How sure are we that this effect size is statistically significant rather than due to random chance?\n\nTo determine this the next step is to run a t-test between the treatment and control groups on the donation outcome. Using the binary variable 'gave'. This designates whether a person gave a donation in any amount.\n\n\n```{python}\nfrom scipy import stats\n\n# Perform both t-tests of 'years'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'gave')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'gave')\nprint('Testing the gave variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n\n```\n\nThe above results show stastical values that lead one to believe the difference in response rates between the treatment and control groups are unlikely to have occurred by chance. It indicates that such financial incentives can effectively influence human behavior, enhancing the likelihood of donating or possibly increasing the donation amounts.\n\n_todo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._\n\n```{python}\nimport statsmodels.formula.api as smf\n\n# Run the probit regression model\ndef run_probit_regression(df, formula):\n    # Probit model using the formula interface\n    model = smf.probit(formula, data=df)\n    results = model.fit(disp=0)\n    return results\n\n# Use the probit regression function\nformula = 'gave ~ treatment'\nprobit_results = run_probit_regression(df, formula)\n\n# Print out the summary of the regression results\ncoefficients = probit_results.params\np_values = probit_results.pvalues\nprint(\"Treatment Coefficient: \", coefficients[1])\nprint(\"Treatment P-Value: \", p_values[1])\n```\n\n### Differences between Match Rates\n\nIn the realm of charitable giving, the size of matched donations often plays a pivotal role in incentivizing potential donors. Understanding how different matching ratios influence donor behavior can provide valuable insights for optimizing fundraising strategies. This analysis focuses on evaluating the impact of various match rates—1:1, 2:1, and 3:1—on the likelihood of donations. By employing a series of t-tests, the study seeks to determine if higher match ratios significantly increase the response rate among donors, thereby testing the assumption that more generous matching offers might lead to higher participation rates in donation campaigns.\n\nBelow is the code to assess the differences in rates of response between different matching offers presented:\n\n```{python}\nfrom scipy import stats\n\n# Calculate the mean donation rate for each ratio category\nmean_ratio1 = df[df['ratio'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Conduct t-tests\nt_test_1_vs_2 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio2'] == 1]['gave'], equal_var=False)\nt_test_1_vs_3 = stats.ttest_ind(df[df['ratio'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\nt_test_2_vs_3 = stats.ttest_ind(df[df['ratio2'] == 1]['gave'], df[df['ratio3'] == 1]['gave'], equal_var=False)\n\n# Print the results\nprint(f\"Mean Donation Rate for 1:1 match: {mean_ratio1:.4f}\")\nprint(f\"Mean Donation Rate for 2:1 match: {mean_ratio2:.4f}\")\nprint(f\"Mean Donation Rate for 3:1 match: {mean_ratio3:.4f}\")\nprint(\"T-test Results:\")\nprint(\"1:1 vs 2:1:\", t_test_1_vs_2)\nprint(\"1:1 vs 3:1:\", t_test_1_vs_3)\nprint(\"2:1 vs 3:1:\", t_test_2_vs_3)\n\n```\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors' comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\n\nNext we'll use regression to look at the comparison of mean response rates for different matching ratios on whether people chose to donate. The coefficient of each variable [ratio1, ratio2, ratio3] can be looked at to determine the effect size between levels of matching presented. The analysis gives us a p-value as well to determine the significance of the movement suggested by the coefficient.\n\n```{python}\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n# Prepare the design matrix X with a constant and the dummy variables for ratio\nX = sm.add_constant(df[['ratio1', 'ratio2', 'ratio3']])\nY = df['gave']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n```\n\n\nInteresting. The summary output will be returned to shortly. Next we'll compare the mean response rates between the matching offers. This will show how much the response rate moves as the offer is increased from 1:1 to 2:1 to 3:1.\n\n```{python}\nmean_ratio1 = df[df['ratio1'] == 1]['gave'].mean()\nmean_ratio2 = df[df['ratio2'] == 1]['gave'].mean()\nmean_ratio3 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Calculate the differences directly from the data\ndifference_1_to_2 = mean_ratio2 - mean_ratio1\ndifference_2_to_3 = mean_ratio3 - mean_ratio2\n\nprint(f\"Directly calculated difference in response rate from 1:1 to 2:1 match ratio: {difference_1_to_2:.4f}\")\nprint(f\"Directly calculated difference in response rate from 2:1 to 3:1 match ratio: {difference_2_to_3:.4f}\")\n```\n\nFor housekeeping we should look to see if the model coefficients show a similar movement to the mean comparisons.\n\n```{python}\ncoefficients = model.params\n\n# Calculate the difference between ratio2 - ratio1 and ratio3 - ratio2\ndifference_ratio2_ratio1 = coefficients['ratio2'] - coefficients['ratio1']\ndifference_ratio3_ratio2 = coefficients['ratio3'] - coefficients['ratio2']\n\ndifference_ratio2_ratio1, difference_ratio3_ratio2\n\nprint(f\"Coefficient comparison difference in response rate from 1:1 to 2:1 match ratio: {difference_ratio2_ratio1:.4f}\")\nprint(f\"Coefficient comparison difference in response rate from 2:1 to 3:1 match ratio: {difference_ratio3_ratio2:.4f}\")\n\n```\n\nThe results of the t-tests reveal that the increase in match ratios from 1:1 to 2:1 and from 1:1 to 3:1 does not result in a statistically significant increase in donation rates, with p-values of 0.334 and 0.310, respectively. This suggests that while there is a slight increase in mean donation rates from 1:1 to 2:1 and 3:1 matches, these differences are not enough to be considered statistically meaningful. Furthermore, the comparison between the 2:1 and 3:1 match ratios, showing a p-value of 0.960, confirms that there is virtually no difference in donor response between these higher match rates. These findings align with the authors' comments that neither the match threshold nor the example donation amount notably affects donor behavior. This suggests that while intuitive expectations might lead one to believe that higher match ratios would significantly enhance donation likelihood due to more attractive incentives, the actual impact on donation behavior may be minimal. This could indicate donor insensitivity to incremental increases in match ratios beyond a certain point, challenging the efficacy of escalating match offers as a strategy to significantly boost donation rates.\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution. In order to do this we shall run the same t-test only this time we're using the donation amount relative to the treatment effect.\n\n```{python}\n\n# Perform both t-tests of 'amount'\nt_stat_direct, df_direct = direct_t_test(df, 'treatment', 'amount')\nt_stat_regression, df_regression = run_regression(df, 'treatment', 'amount')\nprint('Testing the amount variable')\nprint(\"Direct t-test:\", t_stat_direct)\nprint(\"Regression t-test:\", t_stat_regression)\n\n# Calculate the p-value for a two-tailed test\np_value = 2 * (1 - stats.t.cdf(abs(t_stat_direct), df_direct))\n\nprint(\"P-value:\", p_value)\n```\n\nThe analysis suggests that matched donations have a potentially positive, though not statistically significant, impact on donation amounts at the traditional 5% significance level. Given the proximity of the p-value to this threshold, organizations might still consider matched donations as part of a broader, diversified fundraising strategy. Barring additional research, it may yet prove a positive influence on donation amounts.\n\nAs a next step we'll filter the data to only show those who donated something. This can then be compared to the treatment variable to see if treatment has an effect on the amount given that can be teased out of the data. \n\n```{python}\n# Filter to include only rows where a positive donation was made\ndf_donors = df[df['amount'] > 0]\n\n# Prepare the design matrix X with a constant and the treatment indicator\nX = sm.add_constant(df_donors['treatment'])\nY = df_donors['amount']\n\n# Fit the OLS regression model\nmodel = sm.OLS(Y, X).fit()\n\nresults = {param: {'Coefficient': model.params[param], 'P-value': model.pvalues[param]}\n               for param in model.params.keys()}\n\n# Print out the summary of the regression results\nprint(results)\n```\n\nThe coefficient for the treatment variable, which is negative, suggests that being in the treatment group (i.e., offered a matched donation) is associated with a decrease in the donation amount by approximately $1.67 compared to the control group. However, the p-value associated with this coefficient is 0.561, indicating that this effect is not statistically significant.\n\nThe regression analysis reveals that, contrary to expectations, the treatment (matched donations) does not lead to an increase in the amount donated. Instead, there's an indication (though not statistically significant) that it might decrease the amount donated if we are to read the outputs of the regression. All in all, there is not enough information here to say that donation amount specifically is moved by the treatment.\n\n```{python}\n\ntreatment_donated = df_donors[df_donors['treatment'] == 1]\ncontrol_donated = df_donors[df_donors['treatment'] == 0]\n\n# Calculate the mean donation amount for each group\nmean_treatment = treatment_donated['amount'].mean()\nmean_control = control_donated['amount'].mean()\n\n# Creating histograms\nplt.figure(figsize=(14, 6))\n\n# Histogram for treatment group\nplt.subplot(1, 2, 1)\nplt.hist(treatment_donated['amount'], bins=30, color='blue', alpha=0.7)\nplt.axvline(mean_treatment, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Treatment Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_treatment:.2f}', xy=(mean_treatment, 50),\n             xytext=(mean_treatment + 50, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='right')\n\n# Histogram for control group\nplt.subplot(1, 2, 2)\nplt.hist(control_donated['amount'], bins=30, color='green', alpha=0.7)\nplt.axvline(mean_control, color='red', linestyle='dashed', linewidth=3)\nplt.title('Donation Amounts - Control Group')\nplt.xlabel('Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: ${mean_control:.2f}', xy=(mean_control, 20),\n             xytext=(mean_control - 150, 10),\n             arrowprops=dict(facecolor='red', shrink=0.05),\n             horizontalalignment='left')\n\nplt.tight_layout()\nplt.show()\n```\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\nIn the provided text, the author outlines an educational simulation designed to illustrate key statistical principles, namely the Law of Large Numbers and the Central Limit Theorem, using a practical example from a charitable donation context. By setting up a scenario where the probabilities of making a donation differ between respondents who receive a match and those who do not, the simulation aims to show how differences in probabilities influence donation behaviors over a large number of trials. Specifically, respondents who do not receive a match have a lower probability (0.018) of donating compared to those who receive a match (0.022).\n\nThe code below is structured to generate a substantial number of simulations—10,000 for each group—to model donation outcomes according to the specified Bernoulli distributions. It uses Python’s numpy library to simulate these outcomes, ensuring reproducibility by setting a random seed. Once the donation data for both control (no match) and treatment (match) groups are simulated, the script calculates the cumulative average of the differences in donation probabilities between the two groups across the number of draws. This method allows the plot to visually depict how the average differences evolve as more data points are considered, highlighting the convergence behavior predicted by the Law of Large Numbers.\n\n```{python}\n# Set the true probabilities for control and treatment\ntrue_prob_control = 0.018\ntrue_prob_treatment = 0.022\n\n# Number of simulations/draws\nnum_simulations = 10000\n\n# Simulate donations for control and treatment groups\nnp.random.seed(0)  # For reproducibility\ncontrol_donations = np.random.binomial(1, true_prob_control, num_simulations)\ntreatment_donations = np.random.binomial(1, true_prob_treatment, num_simulations)\n\n# Compute the cumulative average of the differences\ncumulative_differences = np.cumsum(treatment_donations - control_donations) / np.arange(1, num_simulations + 1)\n\n# Plot the cumulative averages\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_differences, color='red', lw=2)\nplt.axhline(y=true_prob_treatment - true_prob_control, color='blue', lw=1, linestyle='--')\n\nplt.xlabel('Number of Draws')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Cumulative Average Difference in Donation Rate')\nplt.show()\n\nprint(\"The mean difference is \" + str(true_prob_treatment - true_prob_control) + \" (the blue hash line).\")\n```\n\nThe plot generated by the code visually confirms the theoretical expectation: as the number of simulations increases, the cumulative average of the differences should approach the true difference in means (0.004) between the control and treatment groups. This is depicted on the plot by the blue dashed line at the level of the true mean difference. The exercise not only reinforces the statistical theory behind sampling distributions and their averages but also provides an intuitive grasp of how small probability differences can be detected and quantified with sufficient data, underlining the practical applications of these concepts in analyzing and interpreting data from real-world experiments. This simulation thereby serves as a powerful tool in both teaching and understanding statistical inference through a direct and engaging approach.\n\n### Central Limit Theorem\n\n\nThe Central Limit Theorem (CLT) is a fundamental principle in statistics that plays a pivotal role in the simulation above. It states that, regardless of the distribution of the population, the distribution of the sample means will approximate a normal distribution as the sample size increases, provided the samples are independent and identically distributed with a finite mean and variance. This theorem is crucial because it justifies the use of normal probability theory in the inference about the mean of a population, even when the population itself is not normally distributed.\n\nIn this scenario, the Central Limit Theorem comes into play by ensuring that the distribution of the cumulative average differences between the treatment and control groups will approach a normal distribution as the number of draws (i.e., sample size) increases. Each draw represents a Bernoulli trial where a donation is made with a certain probability. As you simulate more trials, the average of these results (due to the law of large numbers) will converge not only towards the true mean difference but also the distribution of these averages will start to resemble a normal distribution (thanks to the CLT).\n\n```{python}\n# Let's generate the histograms for the sample sizes specified and comment on their distribution.\n\n# Function to simulate the process and calculate the averages\ndef simulate_averages(sample_size, repetitions, p_control, p_treatment):\n    control_means = np.random.binomial(sample_size, p_control, repetitions) / sample_size\n    treatment_means = np.random.binomial(sample_size, p_treatment, repetitions) / sample_size\n    return treatment_means - control_means\n\n# Sample sizes to generate histograms for\nsample_sizes = [50, 200, 500, 1000]\n\n# Control and treatment probabilities\np_control = 0.018\np_treatment = 0.022\n\n# Number of repetitions to calculate averages\nrepetitions = 1000\n\n# Set up the plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# Generate the histograms\nfor i, sample_size in enumerate(sample_sizes):\n    # Simulate the differences in averages\n    avg_differences = simulate_averages(sample_size, repetitions, p_control, p_treatment)\n    \n    # Plot histogram horizontally\n    axes[i].hist(avg_differences, bins=30, orientation='horizontal', color='skyblue', edgecolor='black')\n    mean_value = np.mean(avg_differences)\n    axes[i].axhline(y=mean_value, color='red', linestyle='dashed', linewidth=2)\n    \n    # Annotate the mean difference\n    axes[i].annotate(f'Mean: {mean_value:.4f}', xy=(0.5, mean_value), xytext=(10, 0), \n                     textcoords='offset points', fontsize=10, color='red', ha='center', va='bottom')\n    \n    # Set title and labels\n    axes[i].set_title(f'Sample size: {sample_size}')\n    axes[i].set_ylabel('Average Difference')\n    axes[i].set_xlabel('Frequency')\n\n# Adjust the layout\nplt.tight_layout()\nplt.show()\n```\n\nFor each histogram corresponding to different sample sizes (50, 200, 500, 1000), zero should ideally be within the tails of the distribution if there is no true difference. \n\nSince the true probabilities differ by 0.004 (p_treatment - p_control), zero is not expected to be in the center if the simulation reflects the true difference.\n\nAs sample size increases, the distribution of the average differences should become more centered around the true difference (0.004), and the variance should decrease, making the distribution narrower around the mean. Though there is an element of randomness in each sample, we can see that as n increases the peak of the histogram starts to converge around the mean value we know to be 0.004.\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.506","theme":"cosmo","title":"A Replication of Karlan and List (2007)","author":"Robin Reese","date":"today","callout-appearance":"minimal","code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}