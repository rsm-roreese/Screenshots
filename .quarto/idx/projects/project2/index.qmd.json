{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Robin Reese","date":"today","callout-appearance":"minimal","format":{"html":{"code-fold":true,"code-summary":"Show the code"}}},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\nWe'll start this investigation by loading the data and getting our various tools ready.\n\n```{python}\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in data\ndf = pd.read_csv('blueprinty.csv')\nprint(df.head())\n```\n\nOne of the first items to check is the general distribution of patents held within the firm data. \n\n```{python}\n# Group the data by customer status\ngrouped = df.groupby('iscustomer')\n\n# Plot histograms and add the mean bar as a vertical line\ngrouped['patents'].plot(kind='hist', alpha=0.5, legend=True)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.show()\n\n# Calculate means\nmeans = grouped['patents'].mean()\nprint(means)\n```\n\nAt a glance, it does appear that those firms who are also customers of Blueprinty have a higher average number of patents. From the above we see that non-customers hold roughly 3.6 patents whereas customers hold 4.1 patents on average.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nLet's plot the distribution of ages and location to see if anything stands out.\n\n```{python}\n# Create a new column for age bracket\ndf['agebracket'] = pd.cut(df['age'], bins=range(0, 51, 10), right=False)\n\n# Show the counts by customer status\nsns.countplot(x='agebracket', hue='iscustomer', data=df)\nplt.xlabel('Age Bracket')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Age Bracket')\nplt.show()\n\n# Print the mean customer status by age bracket\nprint(df.groupby('agebracket')['iscustomer'].mean())\n```\n\n```{python}\n# Show the count of each region by customer status\nsns.countplot(x='region', hue='iscustomer', data=df)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Region')\nplt.show()\n\n# print the mean customer status by region\nprint(df.groupby('region')['iscustomer'].mean())\n\n```\n\nThere is definitely a skew in the data. Many firms are within the age range of 10 - 30 years. The region field also suggests that the firms that are also customers are heavily segmented in the Northeast region.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nSimple Poison Model: $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\nGiven the above, the expression\n\n$$\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\log \\left(\\prod_{i=1}^n y_i!\\right)\n$$\n\nis the log-likelihood function for a set of ùëõ observations assumed to be independently and identically distributed according to a Poisson distribution with parameter Œª.\n\nIn Python that math would take the form of the following function:\n\n```{python}\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_ <= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda values\n    # Calculate the log-likelihood\n    try:\n        log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([np.log(math.factorial(y)) for y in Y])\n    except OverflowError:  # Handling OverflowError that can occur with large factorials\n        log_likelihood = float('-inf')\n    return log_likelihood\n```\n\nNow we can use that function to plot the log-likelihood against alues of lambda. The maximum likelihood estimator is shown at the peak of the curve. We can see that it takes the value of approximately 3.68 which you may remember is somewhat closer to the average number of patents of non-customers.\n\n```{python}\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, Y.max() + 2, 400)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]\n\n# Calculate Y_bar, the sample mean of the observed data Y\nY_bar = np.mean(Y)\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.axvline(x=Y_bar, color='red', linestyle='--', label=f'Y_bar at {Y_bar:.2f}', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda Values')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.show()\n```\n\n## Mathematical Derivation\n\nFor a set of observations $( Y = y_1, y_2, \\ldots, y_n )$ that are independently and identically distributed according to a Poisson distribution, the log-likelihood function $\\lambda$ is given by:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right)\n$$\n\nTaking the derivative with respect to $\\lambda$ and setting it to zero:\n\n$$\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( \\frac{y_i}{\\lambda} - 1 \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n$$\n\nSetting this derivative to zero for maximization:\n\n$$\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i = n\n$$\n\n$$\n\\lambda = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{Y}\n$$\n\nThis results in $\\lambda$ being equal to $\\bar{Y}$, which confirms that the mean of the Poisson distribution $\\lambda$ is also the rate parameter that maximizes the likelihood of observing the given data (i.e. the top of the curve on our plot).\n\nWe can also find this peak value by inputting this function into Python's minimize function. Keep in mind that we are trying to maximize the function so instead we'll pass the negative log-likelihood.\n\n```{python}\n# Define the negative log-likelihood function\ndef neg_poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the negative log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_[0] <= 0:\n        return np.inf  # Return infinity if lambda is not positive\n    return -np.sum(Y * np.log(lambda_[0]) - lambda_[0] - gammaln(Y + 1))\n\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Initial guess for lambda\ninitial_lambda = np.array([4.5]) \n\n# Using 'minimize' from scipy.optimize to find the MLE for lambda\nresult = minimize(neg_poisson_loglikelihood, x0=initial_lambda, args=(Y,), bounds=[(0.001, None)])\n\n# The optimal lambda found\nlambda_mle = result.x[0]\n\nprint(\"The MLE for lambda is:\", lambda_mle)\n```\n\nIn solving using this alternative approach we again see a MLE of 3.68.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nThis requires a slight modification to the previous function where the negative log-likelihood was calculated. Below the function is amended to take additional arguments in the form of beta values. These represent the various characteristics of a firm relative to their number of patents.\n\n```{python}\ndef neg_poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"Calculate the negative log-likelihood for the Poisson regression model.\"\"\"\n    linear_pred = np.dot(X, beta)\n    lambda_ = np.exp(linear_pred)\n    log_likelihood = np.sum(-lambda_ + Y * np.log(lambda_) - np.log(np.array([math.factorial(y) for y in Y])))\n    return -log_likelihood  # Return the negative log-likelihood for minimization\n\n```\n\nProcessing of the data to create the regression includes a few steps. First, a column of ones is added to represent the intercept. Second, the square of ages is added as a new column. Third, categorical variables (region) are one-hot encoded to give them numerical values. Finally, these are joined together to create the new dataframe 'X' and all values are converted to integers for ease of calculation. Once done the dataframes will be passed through to minimization function. \n\n```{python}\ndf['age_squared'] = df['age'] ** 2\nX = pd.concat([pd.DataFrame({'intercept': np.ones(len(df))}), df[['age', 'age_squared', 'iscustomer']], pd.get_dummies(df['region'], drop_first=True)], axis=1)\nY = df['patents']\n\n# Cast Y and Y as int\nY = Y.astype(int)\nX = X.astype(int)\n\ncolumn_names = X.columns\n```\n\nThe output shows the feature name, the coefficient for each feature, and its standard error. You may notice that some of the standard errors are significantly larger than the coefficients.\n\n```{python}\n\nscaler = StandardScaler()\n\n# Check if X is a DataFrame and if it has more than one column\nif isinstance(X, pd.DataFrame) and X.shape[1] > 1:\n    # Scale only the non-intercept columns if X is a DataFrame\n    X.iloc[:, 1:] = scaler.fit_transform(X.iloc[:, 1:])\nelse:\n    # If X is already a numpy array or has only one column, handle accordingly\n    print(\"Check the structure of X; it might not be a DataFrame or only contains one column.\")\n\n\n# Initial guess for the parameters (beta)\ninitial_beta = np.zeros(X.shape[1])\n\n# Use 'minimize' to find the MLE of beta\nresult = minimize(\n    neg_poisson_regression_loglikelihood, \n    x0=initial_beta, \n    args=(Y, X), \n    method='L-BFGS-B', \n    options={'disp': True, 'maxcor': 20, 'ftol': 1e-9, 'gtol': 1e-9}\n)\n# Check if the optimization was successful\nif result.success:\n    estimated_beta = result.x\n    print(\"Optimization successful.\")\nelse:\n    print(\"Optimization failed.\")\n    print(result.message)\n\nif 'hess_inv' in result:\n    # Convert the hess_inv to a numpy array if it isn't already\n    hess_inv_matrix = np.array(result.hess_inv.todense()) if not isinstance(result.hess_inv, np.ndarray) else result.hess_inv\n\n    # Compute standard errors\n    std_errors = np.sqrt(np.diag(hess_inv_matrix))\n\n# Create a DataFrame to display results nicely\nresults_df = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': estimated_beta,\n    'Standard Error': std_errors,\n}) \n\nprint(results_df)\n```\n\nLet's check out results again using Python functions that already exist. To better visualize the potential effective sizes given the data, we'll plot the coefficients and their confidence intervals.\n\n```{python}\n\nX = np.asarray(X, dtype=np.int64)\nY = np.asarray(Y, dtype=np.int64)\n\n# Fit the GLM model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n```\n\n\n```{python}\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 2)\n\n# Create a DataFrame to display results nicely\nresults_df_2 = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int_df = pd.DataFrame(conf_int, columns=['Lower CI', 'Upper CI'], index=column_names)\n\n# Merge with the coefficients data\nresults_viz_df = pd.concat([results_df_2.set_index('Variable'), conf_int_df], axis=1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_viz_df.index, data=results_viz_df, capsize=.2)\nfor i, (lower, upper) in enumerate(zip(results_viz_df['Lower CI'], results_viz_df['Upper CI'])):\n    plt.plot([lower, upper], [i, i], color='black')\nplt.title('Effect Sizes with 95% Confidence Intervals')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n```\n\n\n```{python}\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_2.to_string(index=False))\n```\n\nTo determine whether or not being a customer is linked to higher numbers of patents we need to closely examine the final outputs. At first our exploratory data analysis showed there was a noticeable difference in mean number of patents between non-customers and customers.\n\nThe potential effect size as given by the data may not be as large. Our final model suggests a coefficient of 0.049 if 'iscustomer' is true. If we broaden our view of the data we see that the 95% confidence interval is a range of positive values and this makes sense when the standard error is 0.019. \n\nFrom this we can say that the marketing team is right, those using Blueprinty‚Äôs software are more successful in getting their patent applications approved. However, the level of success is potentially so small that one should consider whether the software's use is truely a net benefit. The value of potential patents and cost of software implementation may make this a no-go for some potential users.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\nHere we will assume the number of reviews is a good proxy for the number of bookings. We'll explore the given data and see what effects the available features have on the number of reviews.\n\nTo start we'll load the data and perform some basic exploratory analysis.\n\n```{python}\n# Read in data\nairbnb_data = pd.read_csv('airbnb.csv')\nprint(airbnb_data.shape)\nprint(airbnb_data.head())\n```\n\nIdeally all of the data is a workable data type. Categorical variables should be one-hot encoded and any missing values should be removed.\n\n```{python}\nprint(airbnb_data.isnull().sum())\n```\n```{python}\n# Convert dates\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Drop rows with any missing values in critical columns\nairbnb_data.dropna(subset=['host_since','bathrooms', 'bedrooms', 'price', 'number_of_reviews', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value'], inplace=True)\n\n# Convert 'instant_bookable' to a binary indicator\nairbnb_data['instant_bookable'] = (airbnb_data['instant_bookable'] == 't').astype(int)\n\nprint(airbnb_data.shape)\nprint(airbnb_data.isnull().sum())\n```\n\n```{python}\n# Distribution of number of reviews\nsns.histplot(airbnb_data['number_of_reviews'], kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.show()\n\nprint(airbnb_data['number_of_reviews'].mean())\n```\n\nNow that the data has been cleaned and we have a general sense of the distribution we will attempt to model it using a Poisson Regression. This requires a few steps. First, the features and dependent variable should be split. Second, a constant is needed to service as the intercept. Third, we assure that all data is a common data type. Finally, we fit the model using built-in Python functions to replicate our Poisson regression.\n\n```{python}\n# Prepare the data for modeling\nX = airbnb_data[['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value', 'instant_bookable']]\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\ny = airbnb_data['number_of_reviews']\n\n# Adding constant for statsmodels\nX = sm.add_constant(X)\n\nX = X.astype(int)\ny = y.astype(int)\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 5)\n\n# Create a DataFrame to display results nicely\nresults_df_3 = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_df_3.index, data=results_df_3, capsize=.2)\nplt.title('Effect Sizes')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n```\n\n```{python}\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int.columns = ['Lower CI', 'Upper CI']\n\nresults_df_3 = pd.concat([results_df_3, conf_int], axis=1)\n\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_3.to_string(index=False))\n```\n\nThe plot above shows the general effect size for each of the features. By value the largest effects on the number of reviews are seen in the constant (intercept) with some additional effect by the 'instant_bookable' feature. The table shows each variable (feature), it's associated coefficient, standard error, and p-values as well as the upper and lower 95% confidence intervals.\n\nWhat we see here is statistically significant data, but with coefficients that are so narrowly bounded and close to zero that most of the features do not indicate an effect on the number of reviews. The coefficient of the intercept is very close to the log of the mean value of number of reviews. If the intercept value is close to the log of the mean of the dependent variable, this suggests that the model, without much contribution from other predictors, is essentially reverting to predicting the mean of the count data.\n\nThe approximate relationship can be expressed as $\\log(\\lambda) \\approx \\beta_0$. \n\nWhen other predictors do not significantly influence the model, the intercept alone should closely estimate the log of the mean of the dependent variable.\n\n```{python}\n\n```","srcMarkdownNoYaml":"\n\n\n## Blueprinty Case Study  \n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\nWe'll start this investigation by loading the data and getting our various tools ready.\n\n```{python}\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in data\ndf = pd.read_csv('blueprinty.csv')\nprint(df.head())\n```\n\nOne of the first items to check is the general distribution of patents held within the firm data. \n\n```{python}\n# Group the data by customer status\ngrouped = df.groupby('iscustomer')\n\n# Plot histograms and add the mean bar as a vertical line\ngrouped['patents'].plot(kind='hist', alpha=0.5, legend=True)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.show()\n\n# Calculate means\nmeans = grouped['patents'].mean()\nprint(means)\n```\n\nAt a glance, it does appear that those firms who are also customers of Blueprinty have a higher average number of patents. From the above we see that non-customers hold roughly 3.6 patents whereas customers hold 4.1 patents on average.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nLet's plot the distribution of ages and location to see if anything stands out.\n\n```{python}\n# Create a new column for age bracket\ndf['agebracket'] = pd.cut(df['age'], bins=range(0, 51, 10), right=False)\n\n# Show the counts by customer status\nsns.countplot(x='agebracket', hue='iscustomer', data=df)\nplt.xlabel('Age Bracket')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Age Bracket')\nplt.show()\n\n# Print the mean customer status by age bracket\nprint(df.groupby('agebracket')['iscustomer'].mean())\n```\n\n```{python}\n# Show the count of each region by customer status\nsns.countplot(x='region', hue='iscustomer', data=df)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Count of Customers by Region')\nplt.show()\n\n# print the mean customer status by region\nprint(df.groupby('region')['iscustomer'].mean())\n\n```\n\nThere is definitely a skew in the data. Many firms are within the age range of 10 - 30 years. The region field also suggests that the firms that are also customers are heavily segmented in the Northeast region.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nSimple Poison Model: $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\nGiven the above, the expression\n\n$$\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\log \\left(\\prod_{i=1}^n y_i!\\right)\n$$\n\nis the log-likelihood function for a set of ùëõ observations assumed to be independently and identically distributed according to a Poisson distribution with parameter Œª.\n\nIn Python that math would take the form of the following function:\n\n```{python}\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_ <= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda values\n    # Calculate the log-likelihood\n    try:\n        log_likelihood = -len(Y) * lambda_ + np.sum(Y * np.log(lambda_)) - np.sum([np.log(math.factorial(y)) for y in Y])\n    except OverflowError:  # Handling OverflowError that can occur with large factorials\n        log_likelihood = float('-inf')\n    return log_likelihood\n```\n\nNow we can use that function to plot the log-likelihood against alues of lambda. The maximum likelihood estimator is shown at the peak of the curve. We can see that it takes the value of approximately 3.68 which you may remember is somewhat closer to the average number of patents of non-customers.\n\n```{python}\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, Y.max() + 2, 400)\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]\n\n# Calculate Y_bar, the sample mean of the observed data Y\nY_bar = np.mean(Y)\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.axvline(x=Y_bar, color='red', linestyle='--', label=f'Y_bar at {Y_bar:.2f}', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda Values')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.show()\n```\n\n## Mathematical Derivation\n\nFor a set of observations $( Y = y_1, y_2, \\ldots, y_n )$ that are independently and identically distributed according to a Poisson distribution, the log-likelihood function $\\lambda$ is given by:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right)\n$$\n\nTaking the derivative with respect to $\\lambda$ and setting it to zero:\n\n$$\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( \\frac{y_i}{\\lambda} - 1 \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n$$\n\nSetting this derivative to zero for maximization:\n\n$$\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i = n\n$$\n\n$$\n\\lambda = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{Y}\n$$\n\nThis results in $\\lambda$ being equal to $\\bar{Y}$, which confirms that the mean of the Poisson distribution $\\lambda$ is also the rate parameter that maximizes the likelihood of observing the given data (i.e. the top of the curve on our plot).\n\nWe can also find this peak value by inputting this function into Python's minimize function. Keep in mind that we are trying to maximize the function so instead we'll pass the negative log-likelihood.\n\n```{python}\n# Define the negative log-likelihood function\ndef neg_poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the negative log-likelihood for a Poisson distribution given a parameter lambda and observed data Y.\n    \"\"\"\n    if lambda_[0] <= 0:\n        return np.inf  # Return infinity if lambda is not positive\n    return -np.sum(Y * np.log(lambda_[0]) - lambda_[0] - gammaln(Y + 1))\n\n# Extract the 'patents' column as the observed data Y\nY = df['patents'].values\n\n# Initial guess for lambda\ninitial_lambda = np.array([4.5]) \n\n# Using 'minimize' from scipy.optimize to find the MLE for lambda\nresult = minimize(neg_poisson_loglikelihood, x0=initial_lambda, args=(Y,), bounds=[(0.001, None)])\n\n# The optimal lambda found\nlambda_mle = result.x[0]\n\nprint(\"The MLE for lambda is:\", lambda_mle)\n```\n\nIn solving using this alternative approach we again see a MLE of 3.68.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nThis requires a slight modification to the previous function where the negative log-likelihood was calculated. Below the function is amended to take additional arguments in the form of beta values. These represent the various characteristics of a firm relative to their number of patents.\n\n```{python}\ndef neg_poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"Calculate the negative log-likelihood for the Poisson regression model.\"\"\"\n    linear_pred = np.dot(X, beta)\n    lambda_ = np.exp(linear_pred)\n    log_likelihood = np.sum(-lambda_ + Y * np.log(lambda_) - np.log(np.array([math.factorial(y) for y in Y])))\n    return -log_likelihood  # Return the negative log-likelihood for minimization\n\n```\n\nProcessing of the data to create the regression includes a few steps. First, a column of ones is added to represent the intercept. Second, the square of ages is added as a new column. Third, categorical variables (region) are one-hot encoded to give them numerical values. Finally, these are joined together to create the new dataframe 'X' and all values are converted to integers for ease of calculation. Once done the dataframes will be passed through to minimization function. \n\n```{python}\ndf['age_squared'] = df['age'] ** 2\nX = pd.concat([pd.DataFrame({'intercept': np.ones(len(df))}), df[['age', 'age_squared', 'iscustomer']], pd.get_dummies(df['region'], drop_first=True)], axis=1)\nY = df['patents']\n\n# Cast Y and Y as int\nY = Y.astype(int)\nX = X.astype(int)\n\ncolumn_names = X.columns\n```\n\nThe output shows the feature name, the coefficient for each feature, and its standard error. You may notice that some of the standard errors are significantly larger than the coefficients.\n\n```{python}\n\nscaler = StandardScaler()\n\n# Check if X is a DataFrame and if it has more than one column\nif isinstance(X, pd.DataFrame) and X.shape[1] > 1:\n    # Scale only the non-intercept columns if X is a DataFrame\n    X.iloc[:, 1:] = scaler.fit_transform(X.iloc[:, 1:])\nelse:\n    # If X is already a numpy array or has only one column, handle accordingly\n    print(\"Check the structure of X; it might not be a DataFrame or only contains one column.\")\n\n\n# Initial guess for the parameters (beta)\ninitial_beta = np.zeros(X.shape[1])\n\n# Use 'minimize' to find the MLE of beta\nresult = minimize(\n    neg_poisson_regression_loglikelihood, \n    x0=initial_beta, \n    args=(Y, X), \n    method='L-BFGS-B', \n    options={'disp': True, 'maxcor': 20, 'ftol': 1e-9, 'gtol': 1e-9}\n)\n# Check if the optimization was successful\nif result.success:\n    estimated_beta = result.x\n    print(\"Optimization successful.\")\nelse:\n    print(\"Optimization failed.\")\n    print(result.message)\n\nif 'hess_inv' in result:\n    # Convert the hess_inv to a numpy array if it isn't already\n    hess_inv_matrix = np.array(result.hess_inv.todense()) if not isinstance(result.hess_inv, np.ndarray) else result.hess_inv\n\n    # Compute standard errors\n    std_errors = np.sqrt(np.diag(hess_inv_matrix))\n\n# Create a DataFrame to display results nicely\nresults_df = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': estimated_beta,\n    'Standard Error': std_errors,\n}) \n\nprint(results_df)\n```\n\nLet's check out results again using Python functions that already exist. To better visualize the potential effective sizes given the data, we'll plot the coefficients and their confidence intervals.\n\n```{python}\n\nX = np.asarray(X, dtype=np.int64)\nY = np.asarray(Y, dtype=np.int64)\n\n# Fit the GLM model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n```\n\n\n```{python}\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 2)\n\n# Create a DataFrame to display results nicely\nresults_df_2 = pd.DataFrame({\n    'Variable': column_names,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int_df = pd.DataFrame(conf_int, columns=['Lower CI', 'Upper CI'], index=column_names)\n\n# Merge with the coefficients data\nresults_viz_df = pd.concat([results_df_2.set_index('Variable'), conf_int_df], axis=1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_viz_df.index, data=results_viz_df, capsize=.2)\nfor i, (lower, upper) in enumerate(zip(results_viz_df['Lower CI'], results_viz_df['Upper CI'])):\n    plt.plot([lower, upper], [i, i], color='black')\nplt.title('Effect Sizes with 95% Confidence Intervals')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n```\n\n\n```{python}\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_2.to_string(index=False))\n```\n\nTo determine whether or not being a customer is linked to higher numbers of patents we need to closely examine the final outputs. At first our exploratory data analysis showed there was a noticeable difference in mean number of patents between non-customers and customers.\n\nThe potential effect size as given by the data may not be as large. Our final model suggests a coefficient of 0.049 if 'iscustomer' is true. If we broaden our view of the data we see that the 95% confidence interval is a range of positive values and this makes sense when the standard error is 0.019. \n\nFrom this we can say that the marketing team is right, those using Blueprinty‚Äôs software are more successful in getting their patent applications approved. However, the level of success is potentially so small that one should consider whether the software's use is truely a net benefit. The value of potential patents and cost of software implementation may make this a no-go for some potential users.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\nHere we will assume the number of reviews is a good proxy for the number of bookings. We'll explore the given data and see what effects the available features have on the number of reviews.\n\nTo start we'll load the data and perform some basic exploratory analysis.\n\n```{python}\n# Read in data\nairbnb_data = pd.read_csv('airbnb.csv')\nprint(airbnb_data.shape)\nprint(airbnb_data.head())\n```\n\nIdeally all of the data is a workable data type. Categorical variables should be one-hot encoded and any missing values should be removed.\n\n```{python}\nprint(airbnb_data.isnull().sum())\n```\n```{python}\n# Convert dates\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Drop rows with any missing values in critical columns\nairbnb_data.dropna(subset=['host_since','bathrooms', 'bedrooms', 'price', 'number_of_reviews', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value'], inplace=True)\n\n# Convert 'instant_bookable' to a binary indicator\nairbnb_data['instant_bookable'] = (airbnb_data['instant_bookable'] == 't').astype(int)\n\nprint(airbnb_data.shape)\nprint(airbnb_data.isnull().sum())\n```\n\n```{python}\n# Distribution of number of reviews\nsns.histplot(airbnb_data['number_of_reviews'], kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.show()\n\nprint(airbnb_data['number_of_reviews'].mean())\n```\n\nNow that the data has been cleaned and we have a general sense of the distribution we will attempt to model it using a Poisson Regression. This requires a few steps. First, the features and dependent variable should be split. Second, a constant is needed to service as the intercept. Third, we assure that all data is a common data type. Finally, we fit the model using built-in Python functions to replicate our Poisson regression.\n\n```{python}\n# Prepare the data for modeling\nX = airbnb_data[['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value', 'instant_bookable']]\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\ny = airbnb_data['number_of_reviews']\n\n# Adding constant for statsmodels\nX = sm.add_constant(X)\n\nX = X.astype(int)\ny = y.astype(int)\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\nresult = poisson_model.fit()\n\n# Extract coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse  # Standard errors of the coefficients\n\n# Get p-values\np_values = np.round(result.pvalues, 5)\n\n# Create a DataFrame to display results nicely\nresults_df_3 = pd.DataFrame({\n    'Variable': X.columns,\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-Value': p_values\n})  \n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y=results_df_3.index, data=results_df_3, capsize=.2)\nplt.title('Effect Sizes')\nplt.xlabel('Effect Size (Coefficient)')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axvline(x=0, color='grey', linestyle='--')\nplt.show()\n```\n\n```{python}\n# Compute the confidence intervals\nconf_int = result.conf_int()\nconf_int.columns = ['Lower CI', 'Upper CI']\n\nresults_df_3 = pd.concat([results_df_3, conf_int], axis=1)\n\nprint(\"Model Summary:\\n\")\nprint(\"\\nCoefficient Estimates and Statistics:\")\nprint(results_df_3.to_string(index=False))\n```\n\nThe plot above shows the general effect size for each of the features. By value the largest effects on the number of reviews are seen in the constant (intercept) with some additional effect by the 'instant_bookable' feature. The table shows each variable (feature), it's associated coefficient, standard error, and p-values as well as the upper and lower 95% confidence intervals.\n\nWhat we see here is statistically significant data, but with coefficients that are so narrowly bounded and close to zero that most of the features do not indicate an effect on the number of reviews. The coefficient of the intercept is very close to the log of the mean value of number of reviews. If the intercept value is close to the log of the mean of the dependent variable, this suggests that the model, without much contribution from other predictors, is essentially reverting to predicting the mean of the count data.\n\nThe approximate relationship can be expressed as $\\log(\\lambda) \\approx \\beta_0$. \n\nWhen other predictors do not significantly influence the model, the intercept alone should closely estimate the log of the mean of the dependent variable.\n\n```{python}\n\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.506","theme":"cosmo","title":"Poisson Regression Examples","author":"Robin Reese","date":"today","callout-appearance":"minimal","code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}